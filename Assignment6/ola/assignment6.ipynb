{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7030bacd",
   "metadata": {},
   "source": [
    "<h1>Import frameworks</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ff796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2cea36",
   "metadata": {},
   "source": [
    "<h1>Import datasets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e049cb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset:  60000\n",
      "test dataset:  10000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize pixel values to range [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='Assignment6/', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='Assignment6/', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"train dataset: \", len(train_dataset))\n",
    "print(\"test dataset: \", len(test_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f3f3a",
   "metadata": {},
   "source": [
    "<h1>Plot images</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f8f130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for images, labels in train_loader:\n",
    "    # Plot the images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.imshow(images[i].squeeze(), cmap='gray')  # Assuming images are grayscale\n",
    "        plt.title(f'Label: {labels[i]}')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    break  # Sto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54fb612",
   "metadata": {},
   "source": [
    "<h1>Single hidden layer</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9169f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Epoch [1/10], Step [100/15000], Loss: 1.5697\n",
      "Epoch [1/10], Step [200/15000], Loss: 0.8318\n",
      "Epoch [1/10], Step [300/15000], Loss: 1.5413\n",
      "Epoch [1/10], Step [400/15000], Loss: 0.4903\n",
      "Epoch [1/10], Step [500/15000], Loss: 0.2729\n",
      "Epoch [1/10], Step [600/15000], Loss: 0.1221\n",
      "Epoch [1/10], Step [700/15000], Loss: 0.4982\n",
      "Epoch [1/10], Step [800/15000], Loss: 1.0333\n",
      "Epoch [1/10], Step [900/15000], Loss: 0.8741\n",
      "Epoch [1/10], Step [1000/15000], Loss: 0.8233\n",
      "Epoch [1/10], Step [1100/15000], Loss: 0.0731\n",
      "Epoch [1/10], Step [1200/15000], Loss: 0.1500\n",
      "Epoch [1/10], Step [1300/15000], Loss: 0.2925\n",
      "Epoch [1/10], Step [1400/15000], Loss: 0.1921\n",
      "Epoch [1/10], Step [1500/15000], Loss: 0.0237\n",
      "Epoch [1/10], Step [1600/15000], Loss: 0.1625\n",
      "Epoch [1/10], Step [1700/15000], Loss: 0.3649\n",
      "Epoch [1/10], Step [1800/15000], Loss: 0.4358\n",
      "Epoch [1/10], Step [1900/15000], Loss: 0.4040\n",
      "Epoch [1/10], Step [2000/15000], Loss: 0.2405\n",
      "Epoch [1/10], Step [2100/15000], Loss: 0.3144\n",
      "Epoch [1/10], Step [2200/15000], Loss: 0.0543\n",
      "Epoch [1/10], Step [2300/15000], Loss: 0.1841\n",
      "Epoch [1/10], Step [2400/15000], Loss: 0.0362\n",
      "Epoch [1/10], Step [2500/15000], Loss: 0.6321\n",
      "Epoch [1/10], Step [2600/15000], Loss: 0.0812\n",
      "Epoch [1/10], Step [2700/15000], Loss: 0.0414\n",
      "Epoch [1/10], Step [2800/15000], Loss: 1.6876\n",
      "Epoch [1/10], Step [2900/15000], Loss: 0.8061\n",
      "Epoch [1/10], Step [3000/15000], Loss: 0.9475\n",
      "Epoch [1/10], Step [3100/15000], Loss: 0.2850\n",
      "Epoch [1/10], Step [3200/15000], Loss: 0.0100\n",
      "Epoch [1/10], Step [3300/15000], Loss: 0.1921\n",
      "Epoch [1/10], Step [3400/15000], Loss: 0.7334\n",
      "Epoch [1/10], Step [3500/15000], Loss: 0.6119\n",
      "Epoch [1/10], Step [3600/15000], Loss: 0.0342\n",
      "Epoch [1/10], Step [3700/15000], Loss: 0.6139\n",
      "Epoch [1/10], Step [3800/15000], Loss: 0.0336\n",
      "Epoch [1/10], Step [3900/15000], Loss: 0.0666\n",
      "Epoch [1/10], Step [4000/15000], Loss: 0.3027\n",
      "Epoch [1/10], Step [4100/15000], Loss: 1.2621\n",
      "Epoch [1/10], Step [4200/15000], Loss: 0.1500\n",
      "Epoch [1/10], Step [4300/15000], Loss: 0.2571\n",
      "Epoch [1/10], Step [4400/15000], Loss: 0.4905\n",
      "Epoch [1/10], Step [4500/15000], Loss: 0.5835\n",
      "Epoch [1/10], Step [4600/15000], Loss: 0.0924\n",
      "Epoch [1/10], Step [4700/15000], Loss: 0.0465\n",
      "Epoch [1/10], Step [4800/15000], Loss: 0.0391\n",
      "Epoch [1/10], Step [4900/15000], Loss: 1.2952\n",
      "Epoch [1/10], Step [5000/15000], Loss: 1.0482\n",
      "Epoch [1/10], Step [5100/15000], Loss: 0.2490\n",
      "Epoch [1/10], Step [5200/15000], Loss: 0.1872\n",
      "Epoch [1/10], Step [5300/15000], Loss: 0.0466\n",
      "Epoch [1/10], Step [5400/15000], Loss: 0.0438\n",
      "Epoch [1/10], Step [5500/15000], Loss: 0.0238\n",
      "Epoch [1/10], Step [5600/15000], Loss: 0.0921\n",
      "Epoch [1/10], Step [5700/15000], Loss: 0.0252\n",
      "Epoch [1/10], Step [5800/15000], Loss: 0.0638\n",
      "Epoch [1/10], Step [5900/15000], Loss: 0.1636\n",
      "Epoch [1/10], Step [6000/15000], Loss: 0.3321\n",
      "Epoch [1/10], Step [6100/15000], Loss: 0.9527\n",
      "Epoch [1/10], Step [6200/15000], Loss: 0.0769\n",
      "Epoch [1/10], Step [6300/15000], Loss: 0.0228\n",
      "Epoch [1/10], Step [6400/15000], Loss: 0.7829\n",
      "Epoch [1/10], Step [6500/15000], Loss: 0.0352\n",
      "Epoch [1/10], Step [6600/15000], Loss: 0.2756\n",
      "Epoch [1/10], Step [6700/15000], Loss: 0.0808\n",
      "Epoch [1/10], Step [6800/15000], Loss: 1.2698\n",
      "Epoch [1/10], Step [6900/15000], Loss: 0.3456\n",
      "Epoch [1/10], Step [7000/15000], Loss: 0.6190\n",
      "Epoch [1/10], Step [7100/15000], Loss: 0.0431\n",
      "Epoch [1/10], Step [7200/15000], Loss: 1.0882\n",
      "Epoch [1/10], Step [7300/15000], Loss: 0.4129\n",
      "Epoch [1/10], Step [7400/15000], Loss: 0.3579\n",
      "Epoch [1/10], Step [7500/15000], Loss: 0.0535\n",
      "Epoch [1/10], Step [7600/15000], Loss: 0.0110\n",
      "Epoch [1/10], Step [7700/15000], Loss: 0.0890\n",
      "Epoch [1/10], Step [7800/15000], Loss: 0.0509\n",
      "Epoch [1/10], Step [7900/15000], Loss: 0.1866\n",
      "Epoch [1/10], Step [8000/15000], Loss: 1.1036\n",
      "Epoch [1/10], Step [8100/15000], Loss: 0.0039\n",
      "Epoch [1/10], Step [8200/15000], Loss: 0.0557\n",
      "Epoch [1/10], Step [8300/15000], Loss: 0.5003\n",
      "Epoch [1/10], Step [8400/15000], Loss: 0.1624\n",
      "Epoch [1/10], Step [8500/15000], Loss: 0.0732\n",
      "Epoch [1/10], Step [8600/15000], Loss: 0.0247\n",
      "Epoch [1/10], Step [8700/15000], Loss: 0.6180\n",
      "Epoch [1/10], Step [8800/15000], Loss: 0.0350\n",
      "Epoch [1/10], Step [8900/15000], Loss: 0.2070\n",
      "Epoch [1/10], Step [9000/15000], Loss: 0.8586\n",
      "Epoch [1/10], Step [9100/15000], Loss: 0.5235\n",
      "Epoch [1/10], Step [9200/15000], Loss: 0.1681\n",
      "Epoch [1/10], Step [9300/15000], Loss: 0.0416\n",
      "Epoch [1/10], Step [9400/15000], Loss: 0.0203\n",
      "Epoch [1/10], Step [9500/15000], Loss: 0.0092\n",
      "Epoch [1/10], Step [9600/15000], Loss: 0.0044\n",
      "Epoch [1/10], Step [9700/15000], Loss: 0.0330\n",
      "Epoch [1/10], Step [9800/15000], Loss: 0.7126\n",
      "Epoch [1/10], Step [9900/15000], Loss: 0.0057\n",
      "Epoch [1/10], Step [10000/15000], Loss: 0.0721\n",
      "Epoch [1/10], Step [10100/15000], Loss: 0.0428\n",
      "Epoch [1/10], Step [10200/15000], Loss: 0.0081\n",
      "Epoch [1/10], Step [10300/15000], Loss: 0.0260\n",
      "Epoch [1/10], Step [10400/15000], Loss: 0.0328\n",
      "Epoch [1/10], Step [10500/15000], Loss: 0.4226\n",
      "Epoch [1/10], Step [10600/15000], Loss: 0.0512\n",
      "Epoch [1/10], Step [10700/15000], Loss: 0.2481\n",
      "Epoch [1/10], Step [10800/15000], Loss: 0.0075\n",
      "Epoch [1/10], Step [10900/15000], Loss: 0.2770\n",
      "Epoch [1/10], Step [11000/15000], Loss: 0.0968\n",
      "Epoch [1/10], Step [11100/15000], Loss: 0.0317\n",
      "Epoch [1/10], Step [11200/15000], Loss: 0.2202\n",
      "Epoch [1/10], Step [11300/15000], Loss: 0.0035\n",
      "Epoch [1/10], Step [11400/15000], Loss: 0.1549\n",
      "Epoch [1/10], Step [11500/15000], Loss: 0.1480\n",
      "Epoch [1/10], Step [11600/15000], Loss: 0.0330\n",
      "Epoch [1/10], Step [11700/15000], Loss: 0.0153\n",
      "Epoch [1/10], Step [11800/15000], Loss: 0.5604\n",
      "Epoch [1/10], Step [11900/15000], Loss: 0.0158\n",
      "Epoch [1/10], Step [12000/15000], Loss: 0.0223\n",
      "Epoch [1/10], Step [12100/15000], Loss: 1.5047\n",
      "Epoch [1/10], Step [12200/15000], Loss: 0.0685\n",
      "Epoch [1/10], Step [12300/15000], Loss: 0.2458\n",
      "Epoch [1/10], Step [12400/15000], Loss: 1.9493\n",
      "Epoch [1/10], Step [12500/15000], Loss: 0.2530\n",
      "Epoch [1/10], Step [12600/15000], Loss: 0.2471\n",
      "Epoch [1/10], Step [12700/15000], Loss: 0.0438\n",
      "Epoch [1/10], Step [12800/15000], Loss: 0.0050\n",
      "Epoch [1/10], Step [12900/15000], Loss: 0.0903\n",
      "Epoch [1/10], Step [13000/15000], Loss: 0.4020\n",
      "Epoch [1/10], Step [13100/15000], Loss: 0.3111\n",
      "Epoch [1/10], Step [13200/15000], Loss: 0.0385\n",
      "Epoch [1/10], Step [13300/15000], Loss: 0.0118\n",
      "Epoch [1/10], Step [13400/15000], Loss: 0.0963\n",
      "Epoch [1/10], Step [13500/15000], Loss: 0.2204\n",
      "Epoch [1/10], Step [13600/15000], Loss: 0.0551\n",
      "Epoch [1/10], Step [13700/15000], Loss: 0.0047\n",
      "Epoch [1/10], Step [13800/15000], Loss: 0.2500\n",
      "Epoch [1/10], Step [13900/15000], Loss: 0.0106\n",
      "Epoch [1/10], Step [14000/15000], Loss: 0.1685\n",
      "Epoch [1/10], Step [14100/15000], Loss: 0.7678\n",
      "Epoch [1/10], Step [14200/15000], Loss: 0.6791\n",
      "Epoch [1/10], Step [14300/15000], Loss: 0.0476\n",
      "Epoch [1/10], Step [14400/15000], Loss: 0.5348\n",
      "Epoch [1/10], Step [14500/15000], Loss: 0.0209\n",
      "Epoch [1/10], Step [14600/15000], Loss: 0.7560\n",
      "Epoch [1/10], Step [14700/15000], Loss: 0.0092\n",
      "Epoch [1/10], Step [14800/15000], Loss: 0.6244\n",
      "Epoch [1/10], Step [14900/15000], Loss: 0.0072\n",
      "Epoch [1/10], Step [15000/15000], Loss: 0.3223\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Accuracy of the network on the 10000 test images: 94.6 %\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Epoch [2/10], Step [100/15000], Loss: 0.0222\n",
      "Epoch [2/10], Step [200/15000], Loss: 0.0478\n",
      "Epoch [2/10], Step [300/15000], Loss: 0.0218\n",
      "Epoch [2/10], Step [400/15000], Loss: 0.0549\n",
      "Epoch [2/10], Step [500/15000], Loss: 0.0132\n",
      "Epoch [2/10], Step [600/15000], Loss: 0.0287\n",
      "Epoch [2/10], Step [700/15000], Loss: 0.0115\n",
      "Epoch [2/10], Step [800/15000], Loss: 0.0520\n",
      "Epoch [2/10], Step [900/15000], Loss: 0.0138\n",
      "Epoch [2/10], Step [1000/15000], Loss: 0.1885\n",
      "Epoch [2/10], Step [1100/15000], Loss: 0.2567\n",
      "Epoch [2/10], Step [1200/15000], Loss: 0.0325\n",
      "Epoch [2/10], Step [1300/15000], Loss: 0.2438\n",
      "Epoch [2/10], Step [1400/15000], Loss: 0.0424\n",
      "Epoch [2/10], Step [1500/15000], Loss: 0.4215\n",
      "Epoch [2/10], Step [1600/15000], Loss: 0.1374\n",
      "Epoch [2/10], Step [1700/15000], Loss: 0.0007\n",
      "Epoch [2/10], Step [1800/15000], Loss: 0.1064\n",
      "Epoch [2/10], Step [1900/15000], Loss: 0.0687\n",
      "Epoch [2/10], Step [2000/15000], Loss: 0.0195\n",
      "Epoch [2/10], Step [2100/15000], Loss: 0.0327\n",
      "Epoch [2/10], Step [2200/15000], Loss: 0.0265\n",
      "Epoch [2/10], Step [2300/15000], Loss: 0.6324\n",
      "Epoch [2/10], Step [2400/15000], Loss: 1.9801\n",
      "Epoch [2/10], Step [2500/15000], Loss: 0.0241\n",
      "Epoch [2/10], Step [2600/15000], Loss: 0.6713\n",
      "Epoch [2/10], Step [2700/15000], Loss: 0.4064\n",
      "Epoch [2/10], Step [2800/15000], Loss: 0.4601\n",
      "Epoch [2/10], Step [2900/15000], Loss: 0.1078\n",
      "Epoch [2/10], Step [3000/15000], Loss: 0.0126\n",
      "Epoch [2/10], Step [3100/15000], Loss: 0.0028\n",
      "Epoch [2/10], Step [3200/15000], Loss: 0.0232\n",
      "Epoch [2/10], Step [3300/15000], Loss: 0.0676\n",
      "Epoch [2/10], Step [3400/15000], Loss: 1.0541\n",
      "Epoch [2/10], Step [3500/15000], Loss: 0.0649\n",
      "Epoch [2/10], Step [3600/15000], Loss: 0.0015\n",
      "Epoch [2/10], Step [3700/15000], Loss: 0.0188\n",
      "Epoch [2/10], Step [3800/15000], Loss: 0.1012\n",
      "Epoch [2/10], Step [3900/15000], Loss: 0.1320\n",
      "Epoch [2/10], Step [4000/15000], Loss: 0.0081\n",
      "Epoch [2/10], Step [4100/15000], Loss: 0.1910\n",
      "Epoch [2/10], Step [4200/15000], Loss: 0.0858\n",
      "Epoch [2/10], Step [4300/15000], Loss: 0.0025\n",
      "Epoch [2/10], Step [4400/15000], Loss: 0.4607\n",
      "Epoch [2/10], Step [4500/15000], Loss: 0.0619\n",
      "Epoch [2/10], Step [4600/15000], Loss: 0.1403\n",
      "Epoch [2/10], Step [4700/15000], Loss: 0.0079\n",
      "Epoch [2/10], Step [4800/15000], Loss: 0.0108\n",
      "Epoch [2/10], Step [4900/15000], Loss: 0.0028\n",
      "Epoch [2/10], Step [5000/15000], Loss: 0.0120\n",
      "Epoch [2/10], Step [5100/15000], Loss: 1.6488\n",
      "Epoch [2/10], Step [5200/15000], Loss: 0.0104\n",
      "Epoch [2/10], Step [5300/15000], Loss: 0.0014\n",
      "Epoch [2/10], Step [5400/15000], Loss: 0.0053\n",
      "Epoch [2/10], Step [5500/15000], Loss: 0.0329\n",
      "Epoch [2/10], Step [5600/15000], Loss: 1.9792\n",
      "Epoch [2/10], Step [5700/15000], Loss: 0.0703\n",
      "Epoch [2/10], Step [5800/15000], Loss: 0.0009\n",
      "Epoch [2/10], Step [5900/15000], Loss: 0.0039\n",
      "Epoch [2/10], Step [6000/15000], Loss: 0.0013\n",
      "Epoch [2/10], Step [6100/15000], Loss: 0.1149\n",
      "Epoch [2/10], Step [6200/15000], Loss: 0.1096\n",
      "Epoch [2/10], Step [6300/15000], Loss: 0.0335\n",
      "Epoch [2/10], Step [6400/15000], Loss: 0.2126\n",
      "Epoch [2/10], Step [6500/15000], Loss: 0.0024\n",
      "Epoch [2/10], Step [6600/15000], Loss: 0.0428\n",
      "Epoch [2/10], Step [6700/15000], Loss: 0.0065\n",
      "Epoch [2/10], Step [6800/15000], Loss: 0.0029\n",
      "Epoch [2/10], Step [6900/15000], Loss: 0.4571\n",
      "Epoch [2/10], Step [7000/15000], Loss: 0.0186\n",
      "Epoch [2/10], Step [7100/15000], Loss: 0.0030\n",
      "Epoch [2/10], Step [7200/15000], Loss: 0.0229\n",
      "Epoch [2/10], Step [7300/15000], Loss: 1.2819\n",
      "Epoch [2/10], Step [7400/15000], Loss: 0.2490\n",
      "Epoch [2/10], Step [7500/15000], Loss: 0.1005\n",
      "Epoch [2/10], Step [7600/15000], Loss: 0.0095\n",
      "Epoch [2/10], Step [7700/15000], Loss: 0.1691\n",
      "Epoch [2/10], Step [7800/15000], Loss: 0.3212\n",
      "Epoch [2/10], Step [7900/15000], Loss: 0.0110\n",
      "Epoch [2/10], Step [8000/15000], Loss: 0.0131\n",
      "Epoch [2/10], Step [8100/15000], Loss: 0.0106\n",
      "Epoch [2/10], Step [8200/15000], Loss: 0.0222\n",
      "Epoch [2/10], Step [8300/15000], Loss: 0.3053\n",
      "Epoch [2/10], Step [8400/15000], Loss: 0.0377\n",
      "Epoch [2/10], Step [8500/15000], Loss: 0.7243\n",
      "Epoch [2/10], Step [8600/15000], Loss: 0.0930\n",
      "Epoch [2/10], Step [8700/15000], Loss: 0.9651\n",
      "Epoch [2/10], Step [8800/15000], Loss: 0.0059\n",
      "Epoch [2/10], Step [8900/15000], Loss: 0.0012\n",
      "Epoch [2/10], Step [9000/15000], Loss: 0.0049\n",
      "Epoch [2/10], Step [9100/15000], Loss: 0.0218\n",
      "Epoch [2/10], Step [9200/15000], Loss: 0.0307\n",
      "Epoch [2/10], Step [9300/15000], Loss: 0.9236\n",
      "Epoch [2/10], Step [9400/15000], Loss: 0.0005\n",
      "Epoch [2/10], Step [9500/15000], Loss: 0.0012\n",
      "Epoch [2/10], Step [9600/15000], Loss: 0.0053\n",
      "Epoch [2/10], Step [9700/15000], Loss: 0.0653\n",
      "Epoch [2/10], Step [9800/15000], Loss: 0.0041\n",
      "Epoch [2/10], Step [9900/15000], Loss: 0.0425\n",
      "Epoch [2/10], Step [10000/15000], Loss: 2.6329\n",
      "Epoch [2/10], Step [10100/15000], Loss: 0.1915\n",
      "Epoch [2/10], Step [10200/15000], Loss: 0.0867\n",
      "Epoch [2/10], Step [10300/15000], Loss: 1.0838\n",
      "Epoch [2/10], Step [10400/15000], Loss: 0.0220\n",
      "Epoch [2/10], Step [10500/15000], Loss: 0.0019\n",
      "Epoch [2/10], Step [10600/15000], Loss: 0.0002\n",
      "Epoch [2/10], Step [10700/15000], Loss: 0.0092\n",
      "Epoch [2/10], Step [10800/15000], Loss: 0.4156\n",
      "Epoch [2/10], Step [10900/15000], Loss: 0.3991\n",
      "Epoch [2/10], Step [11000/15000], Loss: 0.0057\n",
      "Epoch [2/10], Step [11100/15000], Loss: 0.0667\n",
      "Epoch [2/10], Step [11200/15000], Loss: 0.1403\n",
      "Epoch [2/10], Step [11300/15000], Loss: 0.0067\n",
      "Epoch [2/10], Step [11400/15000], Loss: 0.0477\n",
      "Epoch [2/10], Step [11500/15000], Loss: 0.1908\n",
      "Epoch [2/10], Step [11600/15000], Loss: 0.0808\n",
      "Epoch [2/10], Step [11700/15000], Loss: 0.0217\n",
      "Epoch [2/10], Step [11800/15000], Loss: 0.2869\n",
      "Epoch [2/10], Step [11900/15000], Loss: 1.1913\n",
      "Epoch [2/10], Step [12000/15000], Loss: 0.0102\n",
      "Epoch [2/10], Step [12100/15000], Loss: 3.5921\n",
      "Epoch [2/10], Step [12200/15000], Loss: 0.4296\n",
      "Epoch [2/10], Step [12300/15000], Loss: 0.0098\n",
      "Epoch [2/10], Step [12400/15000], Loss: 0.0047\n",
      "Epoch [2/10], Step [12500/15000], Loss: 0.0058\n",
      "Epoch [2/10], Step [12600/15000], Loss: 0.1235\n",
      "Epoch [2/10], Step [12700/15000], Loss: 0.0115\n",
      "Epoch [2/10], Step [12800/15000], Loss: 0.0446\n",
      "Epoch [2/10], Step [12900/15000], Loss: 0.1637\n",
      "Epoch [2/10], Step [13000/15000], Loss: 0.0027\n",
      "Epoch [2/10], Step [13100/15000], Loss: 0.0001\n",
      "Epoch [2/10], Step [13200/15000], Loss: 0.0112\n",
      "Epoch [2/10], Step [13300/15000], Loss: 0.0076\n",
      "Epoch [2/10], Step [13400/15000], Loss: 0.0244\n",
      "Epoch [2/10], Step [13500/15000], Loss: 0.0504\n",
      "Epoch [2/10], Step [13600/15000], Loss: 0.0014\n",
      "Epoch [2/10], Step [13700/15000], Loss: 0.0673\n",
      "Epoch [2/10], Step [13800/15000], Loss: 0.1408\n",
      "Epoch [2/10], Step [13900/15000], Loss: 0.0256\n",
      "Epoch [2/10], Step [14000/15000], Loss: 0.0085\n",
      "Epoch [2/10], Step [14100/15000], Loss: 0.0018\n",
      "Epoch [2/10], Step [14200/15000], Loss: 0.1617\n",
      "Epoch [2/10], Step [14300/15000], Loss: 0.0012\n",
      "Epoch [2/10], Step [14400/15000], Loss: 0.0161\n",
      "Epoch [2/10], Step [14500/15000], Loss: 0.1765\n",
      "Epoch [2/10], Step [14600/15000], Loss: 0.0240\n",
      "Epoch [2/10], Step [14700/15000], Loss: 0.0520\n",
      "Epoch [2/10], Step [14800/15000], Loss: 0.4256\n",
      "Epoch [2/10], Step [14900/15000], Loss: 0.0154\n",
      "Epoch [2/10], Step [15000/15000], Loss: 0.0070\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Accuracy of the network on the 10000 test images: 96.42 %\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Epoch [3/10], Step [100/15000], Loss: 0.0022\n",
      "Epoch [3/10], Step [200/15000], Loss: 0.2625\n",
      "Epoch [3/10], Step [300/15000], Loss: 0.1824\n",
      "Epoch [3/10], Step [400/15000], Loss: 0.0061\n",
      "Epoch [3/10], Step [500/15000], Loss: 0.1473\n",
      "Epoch [3/10], Step [600/15000], Loss: 0.0029\n",
      "Epoch [3/10], Step [700/15000], Loss: 0.0004\n",
      "Epoch [3/10], Step [800/15000], Loss: 0.0550\n",
      "Epoch [3/10], Step [900/15000], Loss: 0.1450\n",
      "Epoch [3/10], Step [1000/15000], Loss: 0.2680\n",
      "Epoch [3/10], Step [1100/15000], Loss: 0.1275\n",
      "Epoch [3/10], Step [1200/15000], Loss: 0.5552\n",
      "Epoch [3/10], Step [1300/15000], Loss: 0.1976\n",
      "Epoch [3/10], Step [1400/15000], Loss: 0.8423\n",
      "Epoch [3/10], Step [1500/15000], Loss: 0.0252\n",
      "Epoch [3/10], Step [1600/15000], Loss: 0.6154\n",
      "Epoch [3/10], Step [1700/15000], Loss: 0.0032\n",
      "Epoch [3/10], Step [1800/15000], Loss: 0.0024\n",
      "Epoch [3/10], Step [1900/15000], Loss: 0.0421\n",
      "Epoch [3/10], Step [2000/15000], Loss: 0.0008\n",
      "Epoch [3/10], Step [2100/15000], Loss: 0.4274\n",
      "Epoch [3/10], Step [2200/15000], Loss: 0.6207\n",
      "Epoch [3/10], Step [2300/15000], Loss: 0.0042\n",
      "Epoch [3/10], Step [2400/15000], Loss: 0.0138\n",
      "Epoch [3/10], Step [2500/15000], Loss: 0.0288\n",
      "Epoch [3/10], Step [2600/15000], Loss: 0.0040\n",
      "Epoch [3/10], Step [2700/15000], Loss: 0.0249\n",
      "Epoch [3/10], Step [2800/15000], Loss: 0.1971\n",
      "Epoch [3/10], Step [2900/15000], Loss: 0.0415\n",
      "Epoch [3/10], Step [3000/15000], Loss: 0.0032\n",
      "Epoch [3/10], Step [3100/15000], Loss: 0.0124\n",
      "Epoch [3/10], Step [3200/15000], Loss: 0.0010\n",
      "Epoch [3/10], Step [3300/15000], Loss: 0.0082\n",
      "Epoch [3/10], Step [3400/15000], Loss: 0.0323\n",
      "Epoch [3/10], Step [3500/15000], Loss: 0.0603\n",
      "Epoch [3/10], Step [3600/15000], Loss: 0.0012\n",
      "Epoch [3/10], Step [3700/15000], Loss: 0.0025\n",
      "Epoch [3/10], Step [3800/15000], Loss: 0.0075\n",
      "Epoch [3/10], Step [3900/15000], Loss: 0.0006\n",
      "Epoch [3/10], Step [4000/15000], Loss: 0.0118\n",
      "Epoch [3/10], Step [4100/15000], Loss: 0.0384\n",
      "Epoch [3/10], Step [4200/15000], Loss: 0.2852\n",
      "Epoch [3/10], Step [4300/15000], Loss: 0.0207\n",
      "Epoch [3/10], Step [4400/15000], Loss: 0.0024\n",
      "Epoch [3/10], Step [4500/15000], Loss: 0.0211\n",
      "Epoch [3/10], Step [4600/15000], Loss: 0.0007\n",
      "Epoch [3/10], Step [4700/15000], Loss: 0.0001\n",
      "Epoch [3/10], Step [4800/15000], Loss: 0.9258\n",
      "Epoch [3/10], Step [4900/15000], Loss: 0.0039\n",
      "Epoch [3/10], Step [5000/15000], Loss: 0.0056\n",
      "Epoch [3/10], Step [5100/15000], Loss: 0.1610\n",
      "Epoch [3/10], Step [5200/15000], Loss: 0.0012\n",
      "Epoch [3/10], Step [5300/15000], Loss: 0.0065\n",
      "Epoch [3/10], Step [5400/15000], Loss: 0.1178\n",
      "Epoch [3/10], Step [5500/15000], Loss: 0.0009\n",
      "Epoch [3/10], Step [5600/15000], Loss: 0.0175\n",
      "Epoch [3/10], Step [5700/15000], Loss: 0.0045\n",
      "Epoch [3/10], Step [5800/15000], Loss: 0.0154\n",
      "Epoch [3/10], Step [5900/15000], Loss: 0.0204\n",
      "Epoch [3/10], Step [6000/15000], Loss: 0.2303\n",
      "Epoch [3/10], Step [6100/15000], Loss: 0.0028\n",
      "Epoch [3/10], Step [6200/15000], Loss: 0.0594\n",
      "Epoch [3/10], Step [6300/15000], Loss: 0.5406\n",
      "Epoch [3/10], Step [6400/15000], Loss: 0.5779\n",
      "Epoch [3/10], Step [6500/15000], Loss: 0.0227\n",
      "Epoch [3/10], Step [6600/15000], Loss: 0.0024\n",
      "Epoch [3/10], Step [6700/15000], Loss: 0.0015\n",
      "Epoch [3/10], Step [6800/15000], Loss: 0.2214\n",
      "Epoch [3/10], Step [6900/15000], Loss: 0.0033\n",
      "Epoch [3/10], Step [7000/15000], Loss: 0.0004\n",
      "Epoch [3/10], Step [7100/15000], Loss: 0.9447\n",
      "Epoch [3/10], Step [7200/15000], Loss: 0.0215\n",
      "Epoch [3/10], Step [7300/15000], Loss: 0.1251\n",
      "Epoch [3/10], Step [7400/15000], Loss: 0.0632\n",
      "Epoch [3/10], Step [7500/15000], Loss: 0.0331\n",
      "Epoch [3/10], Step [7600/15000], Loss: 0.7676\n",
      "Epoch [3/10], Step [7700/15000], Loss: 0.0752\n",
      "Epoch [3/10], Step [7800/15000], Loss: 0.0446\n",
      "Epoch [3/10], Step [7900/15000], Loss: 0.0345\n",
      "Epoch [3/10], Step [8000/15000], Loss: 0.0064\n",
      "Epoch [3/10], Step [8100/15000], Loss: 0.0023\n",
      "Epoch [3/10], Step [8200/15000], Loss: 0.0009\n",
      "Epoch [3/10], Step [8300/15000], Loss: 0.0093\n",
      "Epoch [3/10], Step [8400/15000], Loss: 0.0010\n",
      "Epoch [3/10], Step [8500/15000], Loss: 0.0059\n",
      "Epoch [3/10], Step [8600/15000], Loss: 0.0560\n",
      "Epoch [3/10], Step [8700/15000], Loss: 0.1402\n",
      "Epoch [3/10], Step [8800/15000], Loss: 0.0121\n",
      "Epoch [3/10], Step [8900/15000], Loss: 0.0186\n",
      "Epoch [3/10], Step [9000/15000], Loss: 0.0073\n",
      "Epoch [3/10], Step [9100/15000], Loss: 0.0192\n",
      "Epoch [3/10], Step [9200/15000], Loss: 0.0009\n",
      "Epoch [3/10], Step [9300/15000], Loss: 0.0098\n",
      "Epoch [3/10], Step [9400/15000], Loss: 0.0024\n",
      "Epoch [3/10], Step [9500/15000], Loss: 0.0042\n",
      "Epoch [3/10], Step [9600/15000], Loss: 0.0176\n",
      "Epoch [3/10], Step [9700/15000], Loss: 0.0018\n",
      "Epoch [3/10], Step [9800/15000], Loss: 0.0048\n",
      "Epoch [3/10], Step [9900/15000], Loss: 0.0296\n",
      "Epoch [3/10], Step [10000/15000], Loss: 0.0565\n",
      "Epoch [3/10], Step [10100/15000], Loss: 0.0225\n",
      "Epoch [3/10], Step [10200/15000], Loss: 0.0296\n",
      "Epoch [3/10], Step [10300/15000], Loss: 0.0893\n",
      "Epoch [3/10], Step [10400/15000], Loss: 0.0014\n",
      "Epoch [3/10], Step [10500/15000], Loss: 0.0012\n",
      "Epoch [3/10], Step [10600/15000], Loss: 0.0674\n",
      "Epoch [3/10], Step [10700/15000], Loss: 0.2006\n",
      "Epoch [3/10], Step [10800/15000], Loss: 0.0424\n",
      "Epoch [3/10], Step [10900/15000], Loss: 0.0334\n",
      "Epoch [3/10], Step [11000/15000], Loss: 0.0183\n",
      "Epoch [3/10], Step [11100/15000], Loss: 0.0348\n",
      "Epoch [3/10], Step [11200/15000], Loss: 0.0058\n",
      "Epoch [3/10], Step [11300/15000], Loss: 0.0005\n",
      "Epoch [3/10], Step [11400/15000], Loss: 0.1142\n",
      "Epoch [3/10], Step [11500/15000], Loss: 0.0105\n",
      "Epoch [3/10], Step [11600/15000], Loss: 0.4191\n",
      "Epoch [3/10], Step [11700/15000], Loss: 0.8626\n",
      "Epoch [3/10], Step [11800/15000], Loss: 0.0126\n",
      "Epoch [3/10], Step [11900/15000], Loss: 0.0070\n",
      "Epoch [3/10], Step [12000/15000], Loss: 0.0029\n",
      "Epoch [3/10], Step [12100/15000], Loss: 0.0597\n",
      "Epoch [3/10], Step [12200/15000], Loss: 0.0021\n",
      "Epoch [3/10], Step [12300/15000], Loss: 0.0134\n",
      "Epoch [3/10], Step [12400/15000], Loss: 0.0052\n",
      "Epoch [3/10], Step [12500/15000], Loss: 0.0017\n",
      "Epoch [3/10], Step [12600/15000], Loss: 0.2087\n",
      "Epoch [3/10], Step [12700/15000], Loss: 0.0262\n",
      "Epoch [3/10], Step [12800/15000], Loss: 0.0046\n",
      "Epoch [3/10], Step [12900/15000], Loss: 0.0128\n",
      "Epoch [3/10], Step [13000/15000], Loss: 0.0023\n",
      "Epoch [3/10], Step [13100/15000], Loss: 0.0010\n",
      "Epoch [3/10], Step [13200/15000], Loss: 0.0120\n",
      "Epoch [3/10], Step [13300/15000], Loss: 0.1953\n",
      "Epoch [3/10], Step [13400/15000], Loss: 0.1425\n",
      "Epoch [3/10], Step [13500/15000], Loss: 0.0054\n",
      "Epoch [3/10], Step [13600/15000], Loss: 0.0002\n",
      "Epoch [3/10], Step [13700/15000], Loss: 0.0537\n",
      "Epoch [3/10], Step [13800/15000], Loss: 0.4187\n",
      "Epoch [3/10], Step [13900/15000], Loss: 0.0457\n",
      "Epoch [3/10], Step [14000/15000], Loss: 0.1366\n",
      "Epoch [3/10], Step [14100/15000], Loss: 0.0494\n",
      "Epoch [3/10], Step [14200/15000], Loss: 0.8879\n",
      "Epoch [3/10], Step [14300/15000], Loss: 0.0034\n",
      "Epoch [3/10], Step [14400/15000], Loss: 0.0382\n",
      "Epoch [3/10], Step [14500/15000], Loss: 0.0002\n",
      "Epoch [3/10], Step [14600/15000], Loss: 0.0005\n",
      "Epoch [3/10], Step [14700/15000], Loss: 0.1163\n",
      "Epoch [3/10], Step [14800/15000], Loss: 0.1354\n",
      "Epoch [3/10], Step [14900/15000], Loss: 0.0528\n",
      "Epoch [3/10], Step [15000/15000], Loss: 0.0226\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Accuracy of the network on the 10000 test images: 96.98 %\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Epoch [4/10], Step [100/15000], Loss: 0.0142\n",
      "Epoch [4/10], Step [200/15000], Loss: 0.3222\n",
      "Epoch [4/10], Step [300/15000], Loss: 0.0091\n",
      "Epoch [4/10], Step [400/15000], Loss: 0.0002\n",
      "Epoch [4/10], Step [500/15000], Loss: 0.0087\n",
      "Epoch [4/10], Step [600/15000], Loss: 0.3797\n",
      "Epoch [4/10], Step [700/15000], Loss: 0.1250\n",
      "Epoch [4/10], Step [800/15000], Loss: 0.0001\n",
      "Epoch [4/10], Step [900/15000], Loss: 0.0045\n",
      "Epoch [4/10], Step [1000/15000], Loss: 0.0053\n",
      "Epoch [4/10], Step [1100/15000], Loss: 0.0131\n",
      "Epoch [4/10], Step [1200/15000], Loss: 0.0819\n",
      "Epoch [4/10], Step [1300/15000], Loss: 0.0008\n",
      "Epoch [4/10], Step [1400/15000], Loss: 0.0015\n",
      "Epoch [4/10], Step [1500/15000], Loss: 0.5692\n",
      "Epoch [4/10], Step [1600/15000], Loss: 0.0010\n",
      "Epoch [4/10], Step [1700/15000], Loss: 0.0954\n",
      "Epoch [4/10], Step [1800/15000], Loss: 0.0002\n",
      "Epoch [4/10], Step [1900/15000], Loss: 0.4300\n",
      "Epoch [4/10], Step [2000/15000], Loss: 0.0075\n",
      "Epoch [4/10], Step [2100/15000], Loss: 0.0074\n",
      "Epoch [4/10], Step [2200/15000], Loss: 0.0079\n",
      "Epoch [4/10], Step [2300/15000], Loss: 0.0033\n",
      "Epoch [4/10], Step [2400/15000], Loss: 0.0043\n",
      "Epoch [4/10], Step [2500/15000], Loss: 0.0785\n",
      "Epoch [4/10], Step [2600/15000], Loss: 0.0083\n",
      "Epoch [4/10], Step [2700/15000], Loss: 0.0289\n",
      "Epoch [4/10], Step [2800/15000], Loss: 0.0038\n",
      "Epoch [4/10], Step [2900/15000], Loss: 0.0481\n",
      "Epoch [4/10], Step [3000/15000], Loss: 1.0810\n",
      "Epoch [4/10], Step [3100/15000], Loss: 0.0527\n",
      "Epoch [4/10], Step [3200/15000], Loss: 0.0010\n",
      "Epoch [4/10], Step [3300/15000], Loss: 0.0006\n",
      "Epoch [4/10], Step [3400/15000], Loss: 0.3883\n",
      "Epoch [4/10], Step [3500/15000], Loss: 0.0070\n",
      "Epoch [4/10], Step [3600/15000], Loss: 0.0079\n",
      "Epoch [4/10], Step [3700/15000], Loss: 1.8365\n",
      "Epoch [4/10], Step [3800/15000], Loss: 0.0093\n",
      "Epoch [4/10], Step [3900/15000], Loss: 0.0151\n",
      "Epoch [4/10], Step [4000/15000], Loss: 0.0005\n",
      "Epoch [4/10], Step [4100/15000], Loss: 0.0049\n",
      "Epoch [4/10], Step [4200/15000], Loss: 0.1531\n",
      "Epoch [4/10], Step [4300/15000], Loss: 0.0017\n",
      "Epoch [4/10], Step [4400/15000], Loss: 0.0048\n",
      "Epoch [4/10], Step [4500/15000], Loss: 0.0258\n",
      "Epoch [4/10], Step [4600/15000], Loss: 0.0009\n",
      "Epoch [4/10], Step [4700/15000], Loss: 0.0054\n",
      "Epoch [4/10], Step [4800/15000], Loss: 0.4031\n",
      "Epoch [4/10], Step [4900/15000], Loss: 0.0026\n",
      "Epoch [4/10], Step [5000/15000], Loss: 0.0012\n",
      "Epoch [4/10], Step [5100/15000], Loss: 0.0330\n",
      "Epoch [4/10], Step [5200/15000], Loss: 0.8446\n",
      "Epoch [4/10], Step [5300/15000], Loss: 0.0479\n",
      "Epoch [4/10], Step [5400/15000], Loss: 0.4692\n",
      "Epoch [4/10], Step [5500/15000], Loss: 0.0044\n",
      "Epoch [4/10], Step [5600/15000], Loss: 0.0137\n",
      "Epoch [4/10], Step [5700/15000], Loss: 0.0002\n",
      "Epoch [4/10], Step [5800/15000], Loss: 0.2442\n",
      "Epoch [4/10], Step [5900/15000], Loss: 0.0178\n",
      "Epoch [4/10], Step [6000/15000], Loss: 0.1637\n",
      "Epoch [4/10], Step [6100/15000], Loss: 0.0035\n",
      "Epoch [4/10], Step [6200/15000], Loss: 0.0003\n",
      "Epoch [4/10], Step [6300/15000], Loss: 0.0079\n",
      "Epoch [4/10], Step [6400/15000], Loss: 0.0202\n",
      "Epoch [4/10], Step [6500/15000], Loss: 0.0005\n",
      "Epoch [4/10], Step [6600/15000], Loss: 0.0003\n",
      "Epoch [4/10], Step [6700/15000], Loss: 0.0024\n",
      "Epoch [4/10], Step [6800/15000], Loss: 0.0001\n",
      "Epoch [4/10], Step [6900/15000], Loss: 0.0047\n",
      "Epoch [4/10], Step [7000/15000], Loss: 0.0008\n",
      "Epoch [4/10], Step [7100/15000], Loss: 0.0085\n",
      "Epoch [4/10], Step [7200/15000], Loss: 0.1581\n",
      "Epoch [4/10], Step [7300/15000], Loss: 0.0023\n",
      "Epoch [4/10], Step [7400/15000], Loss: 0.0025\n",
      "Epoch [4/10], Step [7500/15000], Loss: 0.0278\n",
      "Epoch [4/10], Step [7600/15000], Loss: 0.0761\n",
      "Epoch [4/10], Step [7700/15000], Loss: 0.0006\n",
      "Epoch [4/10], Step [7800/15000], Loss: 0.1121\n",
      "Epoch [4/10], Step [7900/15000], Loss: 0.0014\n",
      "Epoch [4/10], Step [8000/15000], Loss: 0.0083\n",
      "Epoch [4/10], Step [8100/15000], Loss: 0.0631\n",
      "Epoch [4/10], Step [8200/15000], Loss: 0.0008\n",
      "Epoch [4/10], Step [8300/15000], Loss: 0.0006\n",
      "Epoch [4/10], Step [8400/15000], Loss: 0.0093\n",
      "Epoch [4/10], Step [8500/15000], Loss: 1.6234\n",
      "Epoch [4/10], Step [8600/15000], Loss: 0.0017\n",
      "Epoch [4/10], Step [8700/15000], Loss: 0.0056\n",
      "Epoch [4/10], Step [8800/15000], Loss: 0.2583\n",
      "Epoch [4/10], Step [8900/15000], Loss: 0.1200\n",
      "Epoch [4/10], Step [9000/15000], Loss: 0.0651\n",
      "Epoch [4/10], Step [9100/15000], Loss: 0.0657\n",
      "Epoch [4/10], Step [9200/15000], Loss: 0.0426\n",
      "Epoch [4/10], Step [9300/15000], Loss: 0.0012\n",
      "Epoch [4/10], Step [9400/15000], Loss: 0.0224\n",
      "Epoch [4/10], Step [9500/15000], Loss: 0.0199\n",
      "Epoch [4/10], Step [9600/15000], Loss: 0.8870\n",
      "Epoch [4/10], Step [9700/15000], Loss: 0.2014\n",
      "Epoch [4/10], Step [9800/15000], Loss: 0.0017\n",
      "Epoch [4/10], Step [9900/15000], Loss: 0.0039\n",
      "Epoch [4/10], Step [10000/15000], Loss: 0.0028\n",
      "Epoch [4/10], Step [10100/15000], Loss: 0.2300\n",
      "Epoch [4/10], Step [10200/15000], Loss: 0.0086\n",
      "Epoch [4/10], Step [10300/15000], Loss: 0.2089\n",
      "Epoch [4/10], Step [10400/15000], Loss: 0.0016\n",
      "Epoch [4/10], Step [10500/15000], Loss: 0.0033\n",
      "Epoch [4/10], Step [10600/15000], Loss: 0.0015\n",
      "Epoch [4/10], Step [10700/15000], Loss: 0.0194\n",
      "Epoch [4/10], Step [10800/15000], Loss: 0.0044\n",
      "Epoch [4/10], Step [10900/15000], Loss: 0.0146\n",
      "Epoch [4/10], Step [11000/15000], Loss: 0.1599\n",
      "Epoch [4/10], Step [11100/15000], Loss: 0.0019\n",
      "Epoch [4/10], Step [11200/15000], Loss: 0.0010\n",
      "Epoch [4/10], Step [11300/15000], Loss: 0.0207\n",
      "Epoch [4/10], Step [11400/15000], Loss: 0.0008\n",
      "Epoch [4/10], Step [11500/15000], Loss: 0.0013\n",
      "Epoch [4/10], Step [11600/15000], Loss: 0.0008\n",
      "Epoch [4/10], Step [11700/15000], Loss: 0.0670\n",
      "Epoch [4/10], Step [11800/15000], Loss: 0.0329\n",
      "Epoch [4/10], Step [11900/15000], Loss: 0.0024\n",
      "Epoch [4/10], Step [12000/15000], Loss: 0.5856\n",
      "Epoch [4/10], Step [12100/15000], Loss: 0.0005\n",
      "Epoch [4/10], Step [12200/15000], Loss: 0.0281\n",
      "Epoch [4/10], Step [12300/15000], Loss: 0.0120\n",
      "Epoch [4/10], Step [12400/15000], Loss: 0.0006\n",
      "Epoch [4/10], Step [12500/15000], Loss: 0.0430\n",
      "Epoch [4/10], Step [12600/15000], Loss: 0.2411\n",
      "Epoch [4/10], Step [12700/15000], Loss: 0.0078\n",
      "Epoch [4/10], Step [12800/15000], Loss: 0.0147\n",
      "Epoch [4/10], Step [12900/15000], Loss: 0.0256\n",
      "Epoch [4/10], Step [13000/15000], Loss: 0.0011\n",
      "Epoch [4/10], Step [13100/15000], Loss: 0.0016\n",
      "Epoch [4/10], Step [13200/15000], Loss: 0.6323\n",
      "Epoch [4/10], Step [13300/15000], Loss: 0.0325\n",
      "Epoch [4/10], Step [13400/15000], Loss: 0.0006\n",
      "Epoch [4/10], Step [13500/15000], Loss: 0.0058\n",
      "Epoch [4/10], Step [13600/15000], Loss: 0.0002\n",
      "Epoch [4/10], Step [13700/15000], Loss: 0.0617\n",
      "Epoch [4/10], Step [13800/15000], Loss: 0.0046\n",
      "Epoch [4/10], Step [13900/15000], Loss: 0.0088\n",
      "Epoch [4/10], Step [14000/15000], Loss: 0.0104\n",
      "Epoch [4/10], Step [14100/15000], Loss: 0.0040\n",
      "Epoch [4/10], Step [14200/15000], Loss: 0.0488\n",
      "Epoch [4/10], Step [14300/15000], Loss: 0.8654\n",
      "Epoch [4/10], Step [14400/15000], Loss: 0.3998\n",
      "Epoch [4/10], Step [14500/15000], Loss: 0.0020\n",
      "Epoch [4/10], Step [14600/15000], Loss: 0.2937\n",
      "Epoch [4/10], Step [14700/15000], Loss: 4.5632\n",
      "Epoch [4/10], Step [14800/15000], Loss: 0.2156\n",
      "Epoch [4/10], Step [14900/15000], Loss: 0.0128\n",
      "Epoch [4/10], Step [15000/15000], Loss: 0.1700\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Accuracy of the network on the 10000 test images: 96.95 %\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Epoch [5/10], Step [100/15000], Loss: 0.0009\n",
      "Epoch [5/10], Step [200/15000], Loss: 0.0072\n",
      "Epoch [5/10], Step [300/15000], Loss: 0.0056\n",
      "Epoch [5/10], Step [400/15000], Loss: 0.0032\n",
      "Epoch [5/10], Step [500/15000], Loss: 0.0122\n",
      "Epoch [5/10], Step [600/15000], Loss: 0.0002\n",
      "Epoch [5/10], Step [700/15000], Loss: 0.3493\n",
      "Epoch [5/10], Step [800/15000], Loss: 0.1170\n",
      "Epoch [5/10], Step [900/15000], Loss: 0.0143\n",
      "Epoch [5/10], Step [1000/15000], Loss: 0.0127\n",
      "Epoch [5/10], Step [1100/15000], Loss: 0.0106\n",
      "Epoch [5/10], Step [1200/15000], Loss: 0.0084\n",
      "Epoch [5/10], Step [1300/15000], Loss: 0.0134\n",
      "Epoch [5/10], Step [1400/15000], Loss: 0.0713\n",
      "Epoch [5/10], Step [1500/15000], Loss: 0.0019\n",
      "Epoch [5/10], Step [1600/15000], Loss: 0.0006\n",
      "Epoch [5/10], Step [1700/15000], Loss: 0.0031\n",
      "Epoch [5/10], Step [1800/15000], Loss: 0.0032\n",
      "Epoch [5/10], Step [1900/15000], Loss: 0.0965\n",
      "Epoch [5/10], Step [2000/15000], Loss: 0.0758\n",
      "Epoch [5/10], Step [2100/15000], Loss: 0.0120\n",
      "Epoch [5/10], Step [2200/15000], Loss: 0.0021\n",
      "Epoch [5/10], Step [2300/15000], Loss: 0.0056\n",
      "Epoch [5/10], Step [2400/15000], Loss: 0.0008\n",
      "Epoch [5/10], Step [2500/15000], Loss: 0.0010\n",
      "Epoch [5/10], Step [2600/15000], Loss: 0.0011\n",
      "Epoch [5/10], Step [2700/15000], Loss: 0.7320\n",
      "Epoch [5/10], Step [2800/15000], Loss: 0.0008\n",
      "Epoch [5/10], Step [2900/15000], Loss: 0.0002\n",
      "Epoch [5/10], Step [3000/15000], Loss: 0.0161\n",
      "Epoch [5/10], Step [3100/15000], Loss: 0.0599\n",
      "Epoch [5/10], Step [3200/15000], Loss: 0.0005\n",
      "Epoch [5/10], Step [3300/15000], Loss: 0.0002\n",
      "Epoch [5/10], Step [3400/15000], Loss: 0.0039\n",
      "Epoch [5/10], Step [3500/15000], Loss: 0.0039\n",
      "Epoch [5/10], Step [3600/15000], Loss: 0.0014\n",
      "Epoch [5/10], Step [3700/15000], Loss: 0.0002\n",
      "Epoch [5/10], Step [3800/15000], Loss: 0.0022\n",
      "Epoch [5/10], Step [3900/15000], Loss: 0.0114\n",
      "Epoch [5/10], Step [4000/15000], Loss: 0.0014\n",
      "Epoch [5/10], Step [4100/15000], Loss: 0.0303\n",
      "Epoch [5/10], Step [4200/15000], Loss: 0.0002\n",
      "Epoch [5/10], Step [4300/15000], Loss: 0.0042\n",
      "Epoch [5/10], Step [4400/15000], Loss: 0.0617\n",
      "Epoch [5/10], Step [4500/15000], Loss: 0.0922\n",
      "Epoch [5/10], Step [4600/15000], Loss: 0.0966\n",
      "Epoch [5/10], Step [4700/15000], Loss: 0.0413\n",
      "Epoch [5/10], Step [4800/15000], Loss: 0.0123\n",
      "Epoch [5/10], Step [4900/15000], Loss: 0.0034\n",
      "Epoch [5/10], Step [5000/15000], Loss: 0.0240\n",
      "Epoch [5/10], Step [5100/15000], Loss: 0.0152\n",
      "Epoch [5/10], Step [5200/15000], Loss: 0.0093\n",
      "Epoch [5/10], Step [5300/15000], Loss: 0.6443\n",
      "Epoch [5/10], Step [5400/15000], Loss: 0.0133\n",
      "Epoch [5/10], Step [5500/15000], Loss: 0.0000\n",
      "Epoch [5/10], Step [5600/15000], Loss: 0.0378\n",
      "Epoch [5/10], Step [5700/15000], Loss: 0.3664\n",
      "Epoch [5/10], Step [5800/15000], Loss: 0.0018\n",
      "Epoch [5/10], Step [5900/15000], Loss: 0.0009\n",
      "Epoch [5/10], Step [6000/15000], Loss: 0.1327\n",
      "Epoch [5/10], Step [6100/15000], Loss: 0.0012\n",
      "Epoch [5/10], Step [6200/15000], Loss: 0.1344\n",
      "Epoch [5/10], Step [6300/15000], Loss: 0.0772\n",
      "Epoch [5/10], Step [6400/15000], Loss: 0.0008\n",
      "Epoch [5/10], Step [6500/15000], Loss: 0.0015\n",
      "Epoch [5/10], Step [6600/15000], Loss: 0.0006\n",
      "Epoch [5/10], Step [6700/15000], Loss: 0.0032\n",
      "Epoch [5/10], Step [6800/15000], Loss: 0.0683\n",
      "Epoch [5/10], Step [6900/15000], Loss: 0.0016\n",
      "Epoch [5/10], Step [7000/15000], Loss: 0.0258\n",
      "Epoch [5/10], Step [7100/15000], Loss: 0.0199\n",
      "Epoch [5/10], Step [7200/15000], Loss: 0.0010\n",
      "Epoch [5/10], Step [7300/15000], Loss: 0.8064\n",
      "Epoch [5/10], Step [7400/15000], Loss: 0.0469\n",
      "Epoch [5/10], Step [7500/15000], Loss: 0.0024\n",
      "Epoch [5/10], Step [7600/15000], Loss: 0.0004\n",
      "Epoch [5/10], Step [7700/15000], Loss: 0.0080\n",
      "Epoch [5/10], Step [7800/15000], Loss: 0.0066\n",
      "Epoch [5/10], Step [7900/15000], Loss: 0.0005\n",
      "Epoch [5/10], Step [8000/15000], Loss: 0.0023\n",
      "Epoch [5/10], Step [8100/15000], Loss: 0.0298\n",
      "Epoch [5/10], Step [8200/15000], Loss: 0.0293\n",
      "Epoch [5/10], Step [8300/15000], Loss: 0.0029\n",
      "Epoch [5/10], Step [8400/15000], Loss: 0.0035\n",
      "Epoch [5/10], Step [8500/15000], Loss: 0.0001\n",
      "Epoch [5/10], Step [8600/15000], Loss: 0.0308\n",
      "Epoch [5/10], Step [8700/15000], Loss: 0.1033\n",
      "Epoch [5/10], Step [8800/15000], Loss: 0.0352\n",
      "Epoch [5/10], Step [8900/15000], Loss: 0.1320\n",
      "Epoch [5/10], Step [9000/15000], Loss: 0.4966\n",
      "Epoch [5/10], Step [9100/15000], Loss: 0.0210\n",
      "Epoch [5/10], Step [9200/15000], Loss: 0.0031\n",
      "Epoch [5/10], Step [9300/15000], Loss: 0.0004\n",
      "Epoch [5/10], Step [9400/15000], Loss: 0.5651\n",
      "Epoch [5/10], Step [9500/15000], Loss: 0.0115\n",
      "Epoch [5/10], Step [9600/15000], Loss: 0.0031\n",
      "Epoch [5/10], Step [9700/15000], Loss: 0.0601\n",
      "Epoch [5/10], Step [9800/15000], Loss: 0.4350\n",
      "Epoch [5/10], Step [9900/15000], Loss: 0.0074\n",
      "Epoch [5/10], Step [10000/15000], Loss: 0.5237\n",
      "Epoch [5/10], Step [10100/15000], Loss: 0.0011\n",
      "Epoch [5/10], Step [10200/15000], Loss: 0.0011\n",
      "Epoch [5/10], Step [10300/15000], Loss: 0.0598\n",
      "Epoch [5/10], Step [10400/15000], Loss: 0.0158\n",
      "Epoch [5/10], Step [10500/15000], Loss: 0.0026\n",
      "Epoch [5/10], Step [10600/15000], Loss: 0.0530\n",
      "Epoch [5/10], Step [10700/15000], Loss: 0.0236\n",
      "Epoch [5/10], Step [10800/15000], Loss: 0.0162\n",
      "Epoch [5/10], Step [10900/15000], Loss: 0.0013\n",
      "Epoch [5/10], Step [11000/15000], Loss: 0.0002\n",
      "Epoch [5/10], Step [11100/15000], Loss: 0.3109\n",
      "Epoch [5/10], Step [11200/15000], Loss: 0.0067\n",
      "Epoch [5/10], Step [11300/15000], Loss: 0.1111\n",
      "Epoch [5/10], Step [11400/15000], Loss: 1.0844\n",
      "Epoch [5/10], Step [11500/15000], Loss: 0.0075\n",
      "Epoch [5/10], Step [11600/15000], Loss: 0.0394\n",
      "Epoch [5/10], Step [11700/15000], Loss: 0.1253\n",
      "Epoch [5/10], Step [11800/15000], Loss: 0.0039\n",
      "Epoch [5/10], Step [11900/15000], Loss: 0.0556\n",
      "Epoch [5/10], Step [12000/15000], Loss: 0.0918\n",
      "Epoch [5/10], Step [12100/15000], Loss: 0.0049\n",
      "Epoch [5/10], Step [12200/15000], Loss: 0.0175\n",
      "Epoch [5/10], Step [12300/15000], Loss: 0.0911\n",
      "Epoch [5/10], Step [12400/15000], Loss: 0.0005\n",
      "Epoch [5/10], Step [12500/15000], Loss: 0.0058\n",
      "Epoch [5/10], Step [12600/15000], Loss: 0.0187\n",
      "Epoch [5/10], Step [12700/15000], Loss: 0.2896\n",
      "Epoch [5/10], Step [12800/15000], Loss: 0.0806\n",
      "Epoch [5/10], Step [12900/15000], Loss: 0.0008\n",
      "Epoch [5/10], Step [13000/15000], Loss: 0.0013\n",
      "Epoch [5/10], Step [13100/15000], Loss: 0.6050\n",
      "Epoch [5/10], Step [13200/15000], Loss: 0.0120\n",
      "Epoch [5/10], Step [13300/15000], Loss: 0.0245\n",
      "Epoch [5/10], Step [13400/15000], Loss: 0.1097\n",
      "Epoch [5/10], Step [13500/15000], Loss: 0.0039\n",
      "Epoch [5/10], Step [13600/15000], Loss: 0.0075\n",
      "Epoch [5/10], Step [13700/15000], Loss: 0.0075\n",
      "Epoch [5/10], Step [13800/15000], Loss: 0.0117\n",
      "Epoch [5/10], Step [13900/15000], Loss: 0.1804\n",
      "Epoch [5/10], Step [14000/15000], Loss: 0.0141\n",
      "Epoch [5/10], Step [14100/15000], Loss: 0.0036\n",
      "Epoch [5/10], Step [14200/15000], Loss: 1.9386\n",
      "Epoch [5/10], Step [14300/15000], Loss: 0.0108\n",
      "Epoch [5/10], Step [14400/15000], Loss: 0.0377\n",
      "Epoch [5/10], Step [14500/15000], Loss: 0.0158\n",
      "Epoch [5/10], Step [14600/15000], Loss: 0.0346\n",
      "Epoch [5/10], Step [14700/15000], Loss: 0.0291\n",
      "Epoch [5/10], Step [14800/15000], Loss: 0.0089\n",
      "Epoch [5/10], Step [14900/15000], Loss: 0.0595\n",
      "Epoch [5/10], Step [15000/15000], Loss: 0.0405\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Accuracy of the network on the 10000 test images: 97.53 %\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Epoch [6/10], Step [100/15000], Loss: 0.0020\n",
      "Epoch [6/10], Step [200/15000], Loss: 0.0657\n",
      "Epoch [6/10], Step [300/15000], Loss: 0.2044\n",
      "Epoch [6/10], Step [400/15000], Loss: 0.0312\n",
      "Epoch [6/10], Step [500/15000], Loss: 0.0040\n",
      "Epoch [6/10], Step [600/15000], Loss: 0.0014\n",
      "Epoch [6/10], Step [700/15000], Loss: 0.0008\n",
      "Epoch [6/10], Step [800/15000], Loss: 0.0142\n",
      "Epoch [6/10], Step [900/15000], Loss: 0.0024\n",
      "Epoch [6/10], Step [1000/15000], Loss: 0.0002\n",
      "Epoch [6/10], Step [1100/15000], Loss: 0.0361\n",
      "Epoch [6/10], Step [1200/15000], Loss: 0.0295\n",
      "Epoch [6/10], Step [1300/15000], Loss: 0.0035\n",
      "Epoch [6/10], Step [1400/15000], Loss: 0.0154\n",
      "Epoch [6/10], Step [1500/15000], Loss: 0.0026\n",
      "Epoch [6/10], Step [1600/15000], Loss: 0.2796\n",
      "Epoch [6/10], Step [1700/15000], Loss: 0.0002\n",
      "Epoch [6/10], Step [1800/15000], Loss: 0.0028\n",
      "Epoch [6/10], Step [1900/15000], Loss: 0.0117\n",
      "Epoch [6/10], Step [2000/15000], Loss: 0.0057\n",
      "Epoch [6/10], Step [2100/15000], Loss: 0.0261\n",
      "Epoch [6/10], Step [2200/15000], Loss: 0.0002\n",
      "Epoch [6/10], Step [2300/15000], Loss: 0.0109\n",
      "Epoch [6/10], Step [2400/15000], Loss: 0.2667\n",
      "Epoch [6/10], Step [2500/15000], Loss: 0.0000\n",
      "Epoch [6/10], Step [2600/15000], Loss: 0.0021\n",
      "Epoch [6/10], Step [2700/15000], Loss: 0.0005\n",
      "Epoch [6/10], Step [2800/15000], Loss: 0.0002\n",
      "Epoch [6/10], Step [2900/15000], Loss: 0.0005\n",
      "Epoch [6/10], Step [3000/15000], Loss: 0.0015\n",
      "Epoch [6/10], Step [3100/15000], Loss: 0.0023\n",
      "Epoch [6/10], Step [3200/15000], Loss: 1.0151\n",
      "Epoch [6/10], Step [3300/15000], Loss: 0.0008\n",
      "Epoch [6/10], Step [3400/15000], Loss: 0.0199\n",
      "Epoch [6/10], Step [3500/15000], Loss: 0.0380\n",
      "Epoch [6/10], Step [3600/15000], Loss: 0.0014\n",
      "Epoch [6/10], Step [3700/15000], Loss: 0.0002\n",
      "Epoch [6/10], Step [3800/15000], Loss: 0.0019\n",
      "Epoch [6/10], Step [3900/15000], Loss: 0.0011\n",
      "Epoch [6/10], Step [4000/15000], Loss: 0.0054\n",
      "Epoch [6/10], Step [4100/15000], Loss: 0.0004\n",
      "Epoch [6/10], Step [4200/15000], Loss: 0.0007\n",
      "Epoch [6/10], Step [4300/15000], Loss: 0.0013\n",
      "Epoch [6/10], Step [4400/15000], Loss: 0.1871\n",
      "Epoch [6/10], Step [4500/15000], Loss: 0.0318\n",
      "Epoch [6/10], Step [4600/15000], Loss: 0.0010\n",
      "Epoch [6/10], Step [4700/15000], Loss: 0.0403\n",
      "Epoch [6/10], Step [4800/15000], Loss: 0.0510\n",
      "Epoch [6/10], Step [4900/15000], Loss: 0.0011\n",
      "Epoch [6/10], Step [5000/15000], Loss: 0.0054\n",
      "Epoch [6/10], Step [5100/15000], Loss: 0.5378\n",
      "Epoch [6/10], Step [5200/15000], Loss: 0.0015\n",
      "Epoch [6/10], Step [5300/15000], Loss: 0.0027\n",
      "Epoch [6/10], Step [5400/15000], Loss: 0.0033\n",
      "Epoch [6/10], Step [5500/15000], Loss: 0.0014\n",
      "Epoch [6/10], Step [5600/15000], Loss: 0.0002\n",
      "Epoch [6/10], Step [5700/15000], Loss: 0.0202\n",
      "Epoch [6/10], Step [5800/15000], Loss: 0.4258\n",
      "Epoch [6/10], Step [5900/15000], Loss: 0.0289\n",
      "Epoch [6/10], Step [6000/15000], Loss: 0.0219\n",
      "Epoch [6/10], Step [6100/15000], Loss: 0.0551\n",
      "Epoch [6/10], Step [6200/15000], Loss: 0.0012\n",
      "Epoch [6/10], Step [6300/15000], Loss: 0.0017\n",
      "Epoch [6/10], Step [6400/15000], Loss: 0.0050\n",
      "Epoch [6/10], Step [6500/15000], Loss: 0.0083\n",
      "Epoch [6/10], Step [6600/15000], Loss: 0.0085\n",
      "Epoch [6/10], Step [6700/15000], Loss: 0.0002\n",
      "Epoch [6/10], Step [6800/15000], Loss: 0.0035\n",
      "Epoch [6/10], Step [6900/15000], Loss: 0.0456\n",
      "Epoch [6/10], Step [7000/15000], Loss: 0.0001\n",
      "Epoch [6/10], Step [7100/15000], Loss: 0.2660\n",
      "Epoch [6/10], Step [7200/15000], Loss: 0.0012\n",
      "Epoch [6/10], Step [7300/15000], Loss: 0.0234\n",
      "Epoch [6/10], Step [7400/15000], Loss: 0.1017\n",
      "Epoch [6/10], Step [7500/15000], Loss: 0.4863\n",
      "Epoch [6/10], Step [7600/15000], Loss: 0.0027\n",
      "Epoch [6/10], Step [7700/15000], Loss: 0.0484\n",
      "Epoch [6/10], Step [7800/15000], Loss: 0.0600\n",
      "Epoch [6/10], Step [7900/15000], Loss: 0.0077\n",
      "Epoch [6/10], Step [8000/15000], Loss: 0.0020\n",
      "Epoch [6/10], Step [8100/15000], Loss: 0.0015\n",
      "Epoch [6/10], Step [8200/15000], Loss: 0.1955\n",
      "Epoch [6/10], Step [8300/15000], Loss: 0.0002\n",
      "Epoch [6/10], Step [8400/15000], Loss: 0.0003\n",
      "Epoch [6/10], Step [8500/15000], Loss: 0.1624\n",
      "Epoch [6/10], Step [8600/15000], Loss: 0.0062\n",
      "Epoch [6/10], Step [8700/15000], Loss: 0.0087\n",
      "Epoch [6/10], Step [8800/15000], Loss: 2.0750\n",
      "Epoch [6/10], Step [8900/15000], Loss: 0.0334\n",
      "Epoch [6/10], Step [9000/15000], Loss: 0.0054\n",
      "Epoch [6/10], Step [9100/15000], Loss: 0.0477\n",
      "Epoch [6/10], Step [9200/15000], Loss: 0.0052\n",
      "Epoch [6/10], Step [9300/15000], Loss: 0.0776\n",
      "Epoch [6/10], Step [9400/15000], Loss: 0.0036\n",
      "Epoch [6/10], Step [9500/15000], Loss: 0.0129\n",
      "Epoch [6/10], Step [9600/15000], Loss: 0.0035\n",
      "Epoch [6/10], Step [9700/15000], Loss: 0.0056\n",
      "Epoch [6/10], Step [9800/15000], Loss: 0.0296\n",
      "Epoch [6/10], Step [9900/15000], Loss: 0.7452\n",
      "Epoch [6/10], Step [10000/15000], Loss: 0.1554\n",
      "Epoch [6/10], Step [10100/15000], Loss: 0.0027\n",
      "Epoch [6/10], Step [10200/15000], Loss: 0.0077\n",
      "Epoch [6/10], Step [10300/15000], Loss: 0.0055\n",
      "Epoch [6/10], Step [10400/15000], Loss: 0.0062\n",
      "Epoch [6/10], Step [10500/15000], Loss: 0.0079\n",
      "Epoch [6/10], Step [10600/15000], Loss: 0.2120\n",
      "Epoch [6/10], Step [10700/15000], Loss: 0.0054\n",
      "Epoch [6/10], Step [10800/15000], Loss: 0.0116\n",
      "Epoch [6/10], Step [10900/15000], Loss: 0.0071\n",
      "Epoch [6/10], Step [11000/15000], Loss: 0.0053\n",
      "Epoch [6/10], Step [11100/15000], Loss: 0.0054\n",
      "Epoch [6/10], Step [11200/15000], Loss: 0.0002\n",
      "Epoch [6/10], Step [11300/15000], Loss: 0.0008\n",
      "Epoch [6/10], Step [11400/15000], Loss: 0.0010\n",
      "Epoch [6/10], Step [11500/15000], Loss: 0.1377\n",
      "Epoch [6/10], Step [11600/15000], Loss: 0.0456\n",
      "Epoch [6/10], Step [11700/15000], Loss: 0.2413\n",
      "Epoch [6/10], Step [11800/15000], Loss: 0.0880\n",
      "Epoch [6/10], Step [11900/15000], Loss: 0.0979\n",
      "Epoch [6/10], Step [12000/15000], Loss: 0.0112\n",
      "Epoch [6/10], Step [12100/15000], Loss: 0.0168\n",
      "Epoch [6/10], Step [12200/15000], Loss: 0.0005\n",
      "Epoch [6/10], Step [12300/15000], Loss: 0.0009\n",
      "Epoch [6/10], Step [12400/15000], Loss: 0.0004\n",
      "Epoch [6/10], Step [12500/15000], Loss: 0.0002\n",
      "Epoch [6/10], Step [12600/15000], Loss: 0.0172\n",
      "Epoch [6/10], Step [12700/15000], Loss: 0.0001\n",
      "Epoch [6/10], Step [12800/15000], Loss: 0.0051\n",
      "Epoch [6/10], Step [12900/15000], Loss: 0.0001\n",
      "Epoch [6/10], Step [13000/15000], Loss: 0.0150\n",
      "Epoch [6/10], Step [13100/15000], Loss: 0.0003\n",
      "Epoch [6/10], Step [13200/15000], Loss: 0.0168\n",
      "Epoch [6/10], Step [13300/15000], Loss: 0.0051\n",
      "Epoch [6/10], Step [13400/15000], Loss: 0.0046\n",
      "Epoch [6/10], Step [13500/15000], Loss: 0.0051\n",
      "Epoch [6/10], Step [13600/15000], Loss: 0.0127\n",
      "Epoch [6/10], Step [13700/15000], Loss: 0.0024\n",
      "Epoch [6/10], Step [13800/15000], Loss: 0.0029\n",
      "Epoch [6/10], Step [13900/15000], Loss: 0.0023\n",
      "Epoch [6/10], Step [14000/15000], Loss: 0.0028\n",
      "Epoch [6/10], Step [14100/15000], Loss: 0.0211\n",
      "Epoch [6/10], Step [14200/15000], Loss: 0.0023\n",
      "Epoch [6/10], Step [14300/15000], Loss: 0.0718\n",
      "Epoch [6/10], Step [14400/15000], Loss: 0.0094\n",
      "Epoch [6/10], Step [14500/15000], Loss: 0.0011\n",
      "Epoch [6/10], Step [14600/15000], Loss: 0.1186\n",
      "Epoch [6/10], Step [14700/15000], Loss: 0.0000\n",
      "Epoch [6/10], Step [14800/15000], Loss: 0.0289\n",
      "Epoch [6/10], Step [14900/15000], Loss: 0.0019\n",
      "Epoch [6/10], Step [15000/15000], Loss: 0.0013\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Accuracy of the network on the 10000 test images: 97.25 %\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Epoch [7/10], Step [100/15000], Loss: 0.0049\n",
      "Epoch [7/10], Step [200/15000], Loss: 0.0001\n",
      "Epoch [7/10], Step [300/15000], Loss: 0.0015\n",
      "Epoch [7/10], Step [400/15000], Loss: 0.0067\n",
      "Epoch [7/10], Step [500/15000], Loss: 0.0004\n",
      "Epoch [7/10], Step [600/15000], Loss: 0.0031\n",
      "Epoch [7/10], Step [700/15000], Loss: 0.0184\n",
      "Epoch [7/10], Step [800/15000], Loss: 0.0045\n",
      "Epoch [7/10], Step [900/15000], Loss: 0.0015\n",
      "Epoch [7/10], Step [1000/15000], Loss: 0.0053\n",
      "Epoch [7/10], Step [1100/15000], Loss: 0.2147\n",
      "Epoch [7/10], Step [1200/15000], Loss: 0.0030\n",
      "Epoch [7/10], Step [1300/15000], Loss: 0.0016\n",
      "Epoch [7/10], Step [1400/15000], Loss: 0.0176\n",
      "Epoch [7/10], Step [1500/15000], Loss: 0.1742\n",
      "Epoch [7/10], Step [1600/15000], Loss: 0.0008\n",
      "Epoch [7/10], Step [1700/15000], Loss: 0.0235\n",
      "Epoch [7/10], Step [1800/15000], Loss: 0.0011\n",
      "Epoch [7/10], Step [1900/15000], Loss: 0.9449\n",
      "Epoch [7/10], Step [2000/15000], Loss: 0.0016\n",
      "Epoch [7/10], Step [2100/15000], Loss: 0.0009\n",
      "Epoch [7/10], Step [2200/15000], Loss: 0.0007\n",
      "Epoch [7/10], Step [2300/15000], Loss: 0.0006\n",
      "Epoch [7/10], Step [2400/15000], Loss: 0.5260\n",
      "Epoch [7/10], Step [2500/15000], Loss: 0.2038\n",
      "Epoch [7/10], Step [2600/15000], Loss: 0.0013\n",
      "Epoch [7/10], Step [2700/15000], Loss: 0.0064\n",
      "Epoch [7/10], Step [2800/15000], Loss: 0.0590\n",
      "Epoch [7/10], Step [2900/15000], Loss: 0.2851\n",
      "Epoch [7/10], Step [3000/15000], Loss: 0.0006\n",
      "Epoch [7/10], Step [3100/15000], Loss: 0.0001\n",
      "Epoch [7/10], Step [3200/15000], Loss: 0.0027\n",
      "Epoch [7/10], Step [3300/15000], Loss: 0.0036\n",
      "Epoch [7/10], Step [3400/15000], Loss: 0.0001\n",
      "Epoch [7/10], Step [3500/15000], Loss: 0.0203\n",
      "Epoch [7/10], Step [3600/15000], Loss: 0.0036\n",
      "Epoch [7/10], Step [3700/15000], Loss: 0.0133\n",
      "Epoch [7/10], Step [3800/15000], Loss: 0.0050\n",
      "Epoch [7/10], Step [3900/15000], Loss: 0.0007\n",
      "Epoch [7/10], Step [4000/15000], Loss: 0.0003\n",
      "Epoch [7/10], Step [4100/15000], Loss: 0.0007\n",
      "Epoch [7/10], Step [4200/15000], Loss: 0.0047\n",
      "Epoch [7/10], Step [4300/15000], Loss: 0.0055\n",
      "Epoch [7/10], Step [4400/15000], Loss: 0.3981\n",
      "Epoch [7/10], Step [4500/15000], Loss: 0.0022\n",
      "Epoch [7/10], Step [4600/15000], Loss: 0.0000\n",
      "Epoch [7/10], Step [4700/15000], Loss: 0.0200\n",
      "Epoch [7/10], Step [4800/15000], Loss: 0.0227\n",
      "Epoch [7/10], Step [4900/15000], Loss: 0.0007\n",
      "Epoch [7/10], Step [5000/15000], Loss: 0.0481\n",
      "Epoch [7/10], Step [5100/15000], Loss: 0.0601\n",
      "Epoch [7/10], Step [5200/15000], Loss: 0.0130\n",
      "Epoch [7/10], Step [5300/15000], Loss: 0.0423\n",
      "Epoch [7/10], Step [5400/15000], Loss: 0.3921\n",
      "Epoch [7/10], Step [5500/15000], Loss: 0.0009\n",
      "Epoch [7/10], Step [5600/15000], Loss: 0.0024\n",
      "Epoch [7/10], Step [5700/15000], Loss: 0.0007\n",
      "Epoch [7/10], Step [5800/15000], Loss: 0.2682\n",
      "Epoch [7/10], Step [5900/15000], Loss: 0.1122\n",
      "Epoch [7/10], Step [6000/15000], Loss: 0.0040\n",
      "Epoch [7/10], Step [6100/15000], Loss: 0.7160\n",
      "Epoch [7/10], Step [6200/15000], Loss: 0.6530\n",
      "Epoch [7/10], Step [6300/15000], Loss: 0.0124\n",
      "Epoch [7/10], Step [6400/15000], Loss: 0.0006\n",
      "Epoch [7/10], Step [6500/15000], Loss: 0.0013\n",
      "Epoch [7/10], Step [6600/15000], Loss: 0.0455\n",
      "Epoch [7/10], Step [6700/15000], Loss: 0.0036\n",
      "Epoch [7/10], Step [6800/15000], Loss: 0.0018\n",
      "Epoch [7/10], Step [6900/15000], Loss: 0.1004\n",
      "Epoch [7/10], Step [7000/15000], Loss: 0.0151\n",
      "Epoch [7/10], Step [7100/15000], Loss: 0.0079\n",
      "Epoch [7/10], Step [7200/15000], Loss: 0.0032\n",
      "Epoch [7/10], Step [7300/15000], Loss: 0.0078\n",
      "Epoch [7/10], Step [7400/15000], Loss: 0.2835\n",
      "Epoch [7/10], Step [7500/15000], Loss: 0.0626\n",
      "Epoch [7/10], Step [7600/15000], Loss: 0.0013\n",
      "Epoch [7/10], Step [7700/15000], Loss: 0.0038\n",
      "Epoch [7/10], Step [7800/15000], Loss: 0.0003\n",
      "Epoch [7/10], Step [7900/15000], Loss: 0.0039\n",
      "Epoch [7/10], Step [8000/15000], Loss: 0.7420\n",
      "Epoch [7/10], Step [8100/15000], Loss: 0.0002\n",
      "Epoch [7/10], Step [8200/15000], Loss: 0.1316\n",
      "Epoch [7/10], Step [8300/15000], Loss: 0.0003\n",
      "Epoch [7/10], Step [8400/15000], Loss: 0.0000\n",
      "Epoch [7/10], Step [8500/15000], Loss: 0.0005\n",
      "Epoch [7/10], Step [8600/15000], Loss: 0.0039\n",
      "Epoch [7/10], Step [8700/15000], Loss: 0.4066\n",
      "Epoch [7/10], Step [8800/15000], Loss: 0.0045\n",
      "Epoch [7/10], Step [8900/15000], Loss: 0.0022\n",
      "Epoch [7/10], Step [9000/15000], Loss: 0.0024\n",
      "Epoch [7/10], Step [9100/15000], Loss: 0.0007\n",
      "Epoch [7/10], Step [9200/15000], Loss: 1.1849\n",
      "Epoch [7/10], Step [9300/15000], Loss: 0.1787\n",
      "Epoch [7/10], Step [9400/15000], Loss: 0.0031\n",
      "Epoch [7/10], Step [9500/15000], Loss: 0.0004\n",
      "Epoch [7/10], Step [9600/15000], Loss: 0.0377\n",
      "Epoch [7/10], Step [9700/15000], Loss: 0.0002\n",
      "Epoch [7/10], Step [9800/15000], Loss: 0.0073\n",
      "Epoch [7/10], Step [9900/15000], Loss: 0.0005\n",
      "Epoch [7/10], Step [10000/15000], Loss: 0.0027\n",
      "Epoch [7/10], Step [10100/15000], Loss: 0.0003\n",
      "Epoch [7/10], Step [10200/15000], Loss: 0.0005\n",
      "Epoch [7/10], Step [10300/15000], Loss: 0.0194\n",
      "Epoch [7/10], Step [10400/15000], Loss: 0.1718\n",
      "Epoch [7/10], Step [10500/15000], Loss: 0.0003\n",
      "Epoch [7/10], Step [10600/15000], Loss: 0.0018\n",
      "Epoch [7/10], Step [10700/15000], Loss: 0.0036\n",
      "Epoch [7/10], Step [10800/15000], Loss: 0.0466\n",
      "Epoch [7/10], Step [10900/15000], Loss: 0.0330\n",
      "Epoch [7/10], Step [11000/15000], Loss: 0.0228\n",
      "Epoch [7/10], Step [11100/15000], Loss: 0.0075\n",
      "Epoch [7/10], Step [11200/15000], Loss: 0.0017\n",
      "Epoch [7/10], Step [11300/15000], Loss: 0.0044\n",
      "Epoch [7/10], Step [11400/15000], Loss: 0.0012\n",
      "Epoch [7/10], Step [11500/15000], Loss: 0.0276\n",
      "Epoch [7/10], Step [11600/15000], Loss: 0.0008\n",
      "Epoch [7/10], Step [11700/15000], Loss: 0.0000\n",
      "Epoch [7/10], Step [11800/15000], Loss: 0.0227\n",
      "Epoch [7/10], Step [11900/15000], Loss: 0.0083\n",
      "Epoch [7/10], Step [12000/15000], Loss: 0.0020\n",
      "Epoch [7/10], Step [12100/15000], Loss: 0.0061\n",
      "Epoch [7/10], Step [12200/15000], Loss: 0.0002\n",
      "Epoch [7/10], Step [12300/15000], Loss: 0.1051\n",
      "Epoch [7/10], Step [12400/15000], Loss: 0.0593\n",
      "Epoch [7/10], Step [12500/15000], Loss: 0.0054\n",
      "Epoch [7/10], Step [12600/15000], Loss: 0.1197\n",
      "Epoch [7/10], Step [12700/15000], Loss: 0.0213\n",
      "Epoch [7/10], Step [12800/15000], Loss: 0.0009\n",
      "Epoch [7/10], Step [12900/15000], Loss: 0.0340\n",
      "Epoch [7/10], Step [13000/15000], Loss: 0.0463\n",
      "Epoch [7/10], Step [13100/15000], Loss: 0.0037\n",
      "Epoch [7/10], Step [13200/15000], Loss: 0.0008\n",
      "Epoch [7/10], Step [13300/15000], Loss: 0.0142\n",
      "Epoch [7/10], Step [13400/15000], Loss: 0.4208\n",
      "Epoch [7/10], Step [13500/15000], Loss: 0.0194\n",
      "Epoch [7/10], Step [13600/15000], Loss: 0.0047\n",
      "Epoch [7/10], Step [13700/15000], Loss: 0.0018\n",
      "Epoch [7/10], Step [13800/15000], Loss: 0.0355\n",
      "Epoch [7/10], Step [13900/15000], Loss: 0.0588\n",
      "Epoch [7/10], Step [14000/15000], Loss: 0.0001\n",
      "Epoch [7/10], Step [14100/15000], Loss: 0.0128\n",
      "Epoch [7/10], Step [14200/15000], Loss: 0.0648\n",
      "Epoch [7/10], Step [14300/15000], Loss: 0.0008\n",
      "Epoch [7/10], Step [14400/15000], Loss: 0.0017\n",
      "Epoch [7/10], Step [14500/15000], Loss: 0.0025\n",
      "Epoch [7/10], Step [14600/15000], Loss: 0.0082\n",
      "Epoch [7/10], Step [14700/15000], Loss: 0.0012\n",
      "Epoch [7/10], Step [14800/15000], Loss: 0.0021\n",
      "Epoch [7/10], Step [14900/15000], Loss: 0.0027\n",
      "Epoch [7/10], Step [15000/15000], Loss: 0.0020\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Accuracy of the network on the 10000 test images: 97.32 %\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Epoch [8/10], Step [100/15000], Loss: 0.0155\n",
      "Epoch [8/10], Step [200/15000], Loss: 0.0050\n",
      "Epoch [8/10], Step [300/15000], Loss: 0.0011\n",
      "Epoch [8/10], Step [400/15000], Loss: 0.0431\n",
      "Epoch [8/10], Step [500/15000], Loss: 0.0066\n",
      "Epoch [8/10], Step [600/15000], Loss: 0.0006\n",
      "Epoch [8/10], Step [700/15000], Loss: 0.0018\n",
      "Epoch [8/10], Step [800/15000], Loss: 0.0006\n",
      "Epoch [8/10], Step [900/15000], Loss: 0.0199\n",
      "Epoch [8/10], Step [1000/15000], Loss: 0.0146\n",
      "Epoch [8/10], Step [1100/15000], Loss: 0.0011\n",
      "Epoch [8/10], Step [1200/15000], Loss: 0.0046\n",
      "Epoch [8/10], Step [1300/15000], Loss: 0.0086\n",
      "Epoch [8/10], Step [1400/15000], Loss: 0.0005\n",
      "Epoch [8/10], Step [1500/15000], Loss: 0.0187\n",
      "Epoch [8/10], Step [1600/15000], Loss: 0.0289\n",
      "Epoch [8/10], Step [1700/15000], Loss: 0.8247\n",
      "Epoch [8/10], Step [1800/15000], Loss: 0.0085\n",
      "Epoch [8/10], Step [1900/15000], Loss: 0.0037\n",
      "Epoch [8/10], Step [2000/15000], Loss: 0.0247\n",
      "Epoch [8/10], Step [2100/15000], Loss: 0.0002\n",
      "Epoch [8/10], Step [2200/15000], Loss: 0.0498\n",
      "Epoch [8/10], Step [2300/15000], Loss: 0.0012\n",
      "Epoch [8/10], Step [2400/15000], Loss: 0.0062\n",
      "Epoch [8/10], Step [2500/15000], Loss: 0.0004\n",
      "Epoch [8/10], Step [2600/15000], Loss: 0.0003\n",
      "Epoch [8/10], Step [2700/15000], Loss: 0.1712\n",
      "Epoch [8/10], Step [2800/15000], Loss: 0.0000\n",
      "Epoch [8/10], Step [2900/15000], Loss: 0.0015\n",
      "Epoch [8/10], Step [3000/15000], Loss: 0.0015\n",
      "Epoch [8/10], Step [3100/15000], Loss: 0.0001\n",
      "Epoch [8/10], Step [3200/15000], Loss: 0.0069\n",
      "Epoch [8/10], Step [3300/15000], Loss: 0.0005\n",
      "Epoch [8/10], Step [3400/15000], Loss: 0.0001\n",
      "Epoch [8/10], Step [3500/15000], Loss: 0.2512\n",
      "Epoch [8/10], Step [3600/15000], Loss: 0.1712\n",
      "Epoch [8/10], Step [3700/15000], Loss: 0.0006\n",
      "Epoch [8/10], Step [3800/15000], Loss: 0.5976\n",
      "Epoch [8/10], Step [3900/15000], Loss: 0.2452\n",
      "Epoch [8/10], Step [4000/15000], Loss: 0.0036\n",
      "Epoch [8/10], Step [4100/15000], Loss: 0.0001\n",
      "Epoch [8/10], Step [4200/15000], Loss: 0.0109\n",
      "Epoch [8/10], Step [4300/15000], Loss: 0.0001\n",
      "Epoch [8/10], Step [4400/15000], Loss: 0.0040\n",
      "Epoch [8/10], Step [4500/15000], Loss: 0.0016\n",
      "Epoch [8/10], Step [4600/15000], Loss: 0.0001\n",
      "Epoch [8/10], Step [4700/15000], Loss: 0.0006\n",
      "Epoch [8/10], Step [4800/15000], Loss: 0.0071\n",
      "Epoch [8/10], Step [4900/15000], Loss: 0.0286\n",
      "Epoch [8/10], Step [5000/15000], Loss: 0.0112\n",
      "Epoch [8/10], Step [5100/15000], Loss: 0.0014\n",
      "Epoch [8/10], Step [5200/15000], Loss: 0.0007\n",
      "Epoch [8/10], Step [5300/15000], Loss: 0.0561\n",
      "Epoch [8/10], Step [5400/15000], Loss: 0.0011\n",
      "Epoch [8/10], Step [5500/15000], Loss: 0.0363\n",
      "Epoch [8/10], Step [5600/15000], Loss: 0.0002\n",
      "Epoch [8/10], Step [5700/15000], Loss: 0.0036\n",
      "Epoch [8/10], Step [5800/15000], Loss: 0.0002\n",
      "Epoch [8/10], Step [5900/15000], Loss: 0.0019\n",
      "Epoch [8/10], Step [6000/15000], Loss: 0.0001\n",
      "Epoch [8/10], Step [6100/15000], Loss: 0.0149\n",
      "Epoch [8/10], Step [6200/15000], Loss: 0.0037\n",
      "Epoch [8/10], Step [6300/15000], Loss: 0.0031\n",
      "Epoch [8/10], Step [6400/15000], Loss: 0.0292\n",
      "Epoch [8/10], Step [6500/15000], Loss: 0.2887\n",
      "Epoch [8/10], Step [6600/15000], Loss: 0.0010\n",
      "Epoch [8/10], Step [6700/15000], Loss: 0.0006\n",
      "Epoch [8/10], Step [6800/15000], Loss: 0.0204\n",
      "Epoch [8/10], Step [6900/15000], Loss: 0.0174\n",
      "Epoch [8/10], Step [7000/15000], Loss: 0.0003\n",
      "Epoch [8/10], Step [7100/15000], Loss: 0.0221\n",
      "Epoch [8/10], Step [7200/15000], Loss: 0.0000\n",
      "Epoch [8/10], Step [7300/15000], Loss: 0.0664\n",
      "Epoch [8/10], Step [7400/15000], Loss: 0.0195\n",
      "Epoch [8/10], Step [7500/15000], Loss: 0.0019\n",
      "Epoch [8/10], Step [7600/15000], Loss: 0.0009\n",
      "Epoch [8/10], Step [7700/15000], Loss: 0.0117\n",
      "Epoch [8/10], Step [7800/15000], Loss: 0.0001\n",
      "Epoch [8/10], Step [7900/15000], Loss: 0.0005\n",
      "Epoch [8/10], Step [8000/15000], Loss: 0.0034\n",
      "Epoch [8/10], Step [8100/15000], Loss: 0.0000\n",
      "Epoch [8/10], Step [8200/15000], Loss: 0.0123\n",
      "Epoch [8/10], Step [8300/15000], Loss: 0.0090\n",
      "Epoch [8/10], Step [8400/15000], Loss: 0.0004\n",
      "Epoch [8/10], Step [8500/15000], Loss: 0.0000\n",
      "Epoch [8/10], Step [8600/15000], Loss: 0.0012\n",
      "Epoch [8/10], Step [8700/15000], Loss: 0.0137\n",
      "Epoch [8/10], Step [8800/15000], Loss: 0.0005\n",
      "Epoch [8/10], Step [8900/15000], Loss: 0.0043\n",
      "Epoch [8/10], Step [9000/15000], Loss: 0.0010\n",
      "Epoch [8/10], Step [9100/15000], Loss: 0.0010\n",
      "Epoch [8/10], Step [9200/15000], Loss: 0.0224\n",
      "Epoch [8/10], Step [9300/15000], Loss: 0.0112\n",
      "Epoch [8/10], Step [9400/15000], Loss: 0.0002\n",
      "Epoch [8/10], Step [9500/15000], Loss: 0.0418\n",
      "Epoch [8/10], Step [9600/15000], Loss: 0.0018\n",
      "Epoch [8/10], Step [9700/15000], Loss: 1.5361\n",
      "Epoch [8/10], Step [9800/15000], Loss: 0.0003\n",
      "Epoch [8/10], Step [9900/15000], Loss: 0.1439\n",
      "Epoch [8/10], Step [10000/15000], Loss: 0.0004\n",
      "Epoch [8/10], Step [10100/15000], Loss: 0.0003\n",
      "Epoch [8/10], Step [10200/15000], Loss: 0.0010\n",
      "Epoch [8/10], Step [10300/15000], Loss: 0.0853\n",
      "Epoch [8/10], Step [10400/15000], Loss: 0.0028\n",
      "Epoch [8/10], Step [10500/15000], Loss: 0.0029\n",
      "Epoch [8/10], Step [10600/15000], Loss: 0.0036\n",
      "Epoch [8/10], Step [10700/15000], Loss: 0.0003\n",
      "Epoch [8/10], Step [10800/15000], Loss: 0.0024\n",
      "Epoch [8/10], Step [10900/15000], Loss: 0.0035\n",
      "Epoch [8/10], Step [11000/15000], Loss: 0.0571\n",
      "Epoch [8/10], Step [11100/15000], Loss: 0.0843\n",
      "Epoch [8/10], Step [11200/15000], Loss: 0.0099\n",
      "Epoch [8/10], Step [11300/15000], Loss: 0.0002\n",
      "Epoch [8/10], Step [11400/15000], Loss: 0.0177\n",
      "Epoch [8/10], Step [11500/15000], Loss: 0.0001\n",
      "Epoch [8/10], Step [11600/15000], Loss: 0.0001\n",
      "Epoch [8/10], Step [11700/15000], Loss: 0.0134\n",
      "Epoch [8/10], Step [11800/15000], Loss: 0.0123\n",
      "Epoch [8/10], Step [11900/15000], Loss: 0.0000\n",
      "Epoch [8/10], Step [12000/15000], Loss: 0.0040\n",
      "Epoch [8/10], Step [12100/15000], Loss: 0.0141\n",
      "Epoch [8/10], Step [12200/15000], Loss: 0.0032\n",
      "Epoch [8/10], Step [12300/15000], Loss: 0.0017\n",
      "Epoch [8/10], Step [12400/15000], Loss: 0.0022\n",
      "Epoch [8/10], Step [12500/15000], Loss: 0.0009\n",
      "Epoch [8/10], Step [12600/15000], Loss: 0.0015\n",
      "Epoch [8/10], Step [12700/15000], Loss: 0.0148\n",
      "Epoch [8/10], Step [12800/15000], Loss: 0.0036\n",
      "Epoch [8/10], Step [12900/15000], Loss: 0.0168\n",
      "Epoch [8/10], Step [13000/15000], Loss: 0.0005\n",
      "Epoch [8/10], Step [13100/15000], Loss: 0.0022\n",
      "Epoch [8/10], Step [13200/15000], Loss: 0.0011\n",
      "Epoch [8/10], Step [13300/15000], Loss: 0.0009\n",
      "Epoch [8/10], Step [13400/15000], Loss: 0.0034\n",
      "Epoch [8/10], Step [13500/15000], Loss: 0.0241\n",
      "Epoch [8/10], Step [13600/15000], Loss: 0.0164\n",
      "Epoch [8/10], Step [13700/15000], Loss: 0.0002\n",
      "Epoch [8/10], Step [13800/15000], Loss: 0.0064\n",
      "Epoch [8/10], Step [13900/15000], Loss: 0.0002\n",
      "Epoch [8/10], Step [14000/15000], Loss: 0.0017\n",
      "Epoch [8/10], Step [14100/15000], Loss: 0.3685\n",
      "Epoch [8/10], Step [14200/15000], Loss: 0.0060\n",
      "Epoch [8/10], Step [14300/15000], Loss: 0.0005\n",
      "Epoch [8/10], Step [14400/15000], Loss: 0.9082\n",
      "Epoch [8/10], Step [14500/15000], Loss: 0.0073\n",
      "Epoch [8/10], Step [14600/15000], Loss: 0.0007\n",
      "Epoch [8/10], Step [14700/15000], Loss: 0.0159\n",
      "Epoch [8/10], Step [14800/15000], Loss: 0.0024\n",
      "Epoch [8/10], Step [14900/15000], Loss: 0.0002\n",
      "Epoch [8/10], Step [15000/15000], Loss: 0.0014\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Accuracy of the network on the 10000 test images: 97.86 %\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Epoch [9/10], Step [100/15000], Loss: 0.0084\n",
      "Epoch [9/10], Step [200/15000], Loss: 0.0173\n",
      "Epoch [9/10], Step [300/15000], Loss: 0.0010\n",
      "Epoch [9/10], Step [400/15000], Loss: 0.0000\n",
      "Epoch [9/10], Step [500/15000], Loss: 0.0088\n",
      "Epoch [9/10], Step [600/15000], Loss: 0.0000\n",
      "Epoch [9/10], Step [700/15000], Loss: 0.0000\n",
      "Epoch [9/10], Step [800/15000], Loss: 0.0001\n",
      "Epoch [9/10], Step [900/15000], Loss: 0.5337\n",
      "Epoch [9/10], Step [1000/15000], Loss: 0.0493\n",
      "Epoch [9/10], Step [1100/15000], Loss: 0.0000\n",
      "Epoch [9/10], Step [1200/15000], Loss: 0.0019\n",
      "Epoch [9/10], Step [1300/15000], Loss: 0.0001\n",
      "Epoch [9/10], Step [1400/15000], Loss: 0.0002\n",
      "Epoch [9/10], Step [1500/15000], Loss: 0.0004\n",
      "Epoch [9/10], Step [1600/15000], Loss: 0.0250\n",
      "Epoch [9/10], Step [1700/15000], Loss: 0.0003\n",
      "Epoch [9/10], Step [1800/15000], Loss: 0.0035\n",
      "Epoch [9/10], Step [1900/15000], Loss: 0.0010\n",
      "Epoch [9/10], Step [2000/15000], Loss: 0.0010\n",
      "Epoch [9/10], Step [2100/15000], Loss: 0.0167\n",
      "Epoch [9/10], Step [2200/15000], Loss: 0.0722\n",
      "Epoch [9/10], Step [2300/15000], Loss: 0.0038\n",
      "Epoch [9/10], Step [2400/15000], Loss: 0.0183\n",
      "Epoch [9/10], Step [2500/15000], Loss: 0.0077\n",
      "Epoch [9/10], Step [2600/15000], Loss: 0.0077\n",
      "Epoch [9/10], Step [2700/15000], Loss: 0.0308\n",
      "Epoch [9/10], Step [2800/15000], Loss: 0.0569\n",
      "Epoch [9/10], Step [2900/15000], Loss: 0.3027\n",
      "Epoch [9/10], Step [3000/15000], Loss: 0.0032\n",
      "Epoch [9/10], Step [3100/15000], Loss: 0.0259\n",
      "Epoch [9/10], Step [3200/15000], Loss: 0.0015\n",
      "Epoch [9/10], Step [3300/15000], Loss: 0.0001\n",
      "Epoch [9/10], Step [3400/15000], Loss: 0.0023\n",
      "Epoch [9/10], Step [3500/15000], Loss: 0.0164\n",
      "Epoch [9/10], Step [3600/15000], Loss: 0.0028\n",
      "Epoch [9/10], Step [3700/15000], Loss: 0.0383\n",
      "Epoch [9/10], Step [3800/15000], Loss: 0.0002\n",
      "Epoch [9/10], Step [3900/15000], Loss: 0.0005\n",
      "Epoch [9/10], Step [4000/15000], Loss: 0.6417\n",
      "Epoch [9/10], Step [4100/15000], Loss: 0.0582\n",
      "Epoch [9/10], Step [4200/15000], Loss: 0.0004\n",
      "Epoch [9/10], Step [4300/15000], Loss: 0.0003\n",
      "Epoch [9/10], Step [4400/15000], Loss: 0.0000\n",
      "Epoch [9/10], Step [4500/15000], Loss: 0.0065\n",
      "Epoch [9/10], Step [4600/15000], Loss: 0.0329\n",
      "Epoch [9/10], Step [4700/15000], Loss: 0.0329\n",
      "Epoch [9/10], Step [4800/15000], Loss: 0.0017\n",
      "Epoch [9/10], Step [4900/15000], Loss: 0.0012\n",
      "Epoch [9/10], Step [5000/15000], Loss: 0.0067\n",
      "Epoch [9/10], Step [5100/15000], Loss: 0.1849\n",
      "Epoch [9/10], Step [5200/15000], Loss: 0.0002\n",
      "Epoch [9/10], Step [5300/15000], Loss: 0.0063\n",
      "Epoch [9/10], Step [5400/15000], Loss: 0.0111\n",
      "Epoch [9/10], Step [5500/15000], Loss: 0.0000\n",
      "Epoch [9/10], Step [5600/15000], Loss: 0.1195\n",
      "Epoch [9/10], Step [5700/15000], Loss: 0.0015\n",
      "Epoch [9/10], Step [5800/15000], Loss: 0.0105\n",
      "Epoch [9/10], Step [5900/15000], Loss: 0.0004\n",
      "Epoch [9/10], Step [6000/15000], Loss: 0.3198\n",
      "Epoch [9/10], Step [6100/15000], Loss: 0.0002\n",
      "Epoch [9/10], Step [6200/15000], Loss: 0.2907\n",
      "Epoch [9/10], Step [6300/15000], Loss: 0.0002\n",
      "Epoch [9/10], Step [6400/15000], Loss: 0.0003\n",
      "Epoch [9/10], Step [6500/15000], Loss: 0.0021\n",
      "Epoch [9/10], Step [6600/15000], Loss: 0.0065\n",
      "Epoch [9/10], Step [6700/15000], Loss: 0.0038\n",
      "Epoch [9/10], Step [6800/15000], Loss: 0.0001\n",
      "Epoch [9/10], Step [6900/15000], Loss: 0.0007\n",
      "Epoch [9/10], Step [7000/15000], Loss: 0.1549\n",
      "Epoch [9/10], Step [7100/15000], Loss: 0.0026\n",
      "Epoch [9/10], Step [7200/15000], Loss: 0.1815\n",
      "Epoch [9/10], Step [7300/15000], Loss: 0.0057\n",
      "Epoch [9/10], Step [7400/15000], Loss: 0.0004\n",
      "Epoch [9/10], Step [7500/15000], Loss: 0.0118\n",
      "Epoch [9/10], Step [7600/15000], Loss: 0.0909\n",
      "Epoch [9/10], Step [7700/15000], Loss: 0.0477\n",
      "Epoch [9/10], Step [7800/15000], Loss: 0.0713\n",
      "Epoch [9/10], Step [7900/15000], Loss: 0.0002\n",
      "Epoch [9/10], Step [8000/15000], Loss: 0.0000\n",
      "Epoch [9/10], Step [8100/15000], Loss: 0.0024\n",
      "Epoch [9/10], Step [8200/15000], Loss: 0.0205\n",
      "Epoch [9/10], Step [8300/15000], Loss: 0.0013\n",
      "Epoch [9/10], Step [8400/15000], Loss: 0.0049\n",
      "Epoch [9/10], Step [8500/15000], Loss: 0.5544\n",
      "Epoch [9/10], Step [8600/15000], Loss: 0.0026\n",
      "Epoch [9/10], Step [8700/15000], Loss: 0.0005\n",
      "Epoch [9/10], Step [8800/15000], Loss: 0.5218\n",
      "Epoch [9/10], Step [8900/15000], Loss: 0.0025\n",
      "Epoch [9/10], Step [9000/15000], Loss: 0.0062\n",
      "Epoch [9/10], Step [9100/15000], Loss: 0.0907\n",
      "Epoch [9/10], Step [9200/15000], Loss: 0.0005\n",
      "Epoch [9/10], Step [9300/15000], Loss: 0.0101\n",
      "Epoch [9/10], Step [9400/15000], Loss: 0.0456\n",
      "Epoch [9/10], Step [9500/15000], Loss: 0.0024\n",
      "Epoch [9/10], Step [9600/15000], Loss: 0.0004\n",
      "Epoch [9/10], Step [9700/15000], Loss: 0.0019\n",
      "Epoch [9/10], Step [9800/15000], Loss: 0.1778\n",
      "Epoch [9/10], Step [9900/15000], Loss: 0.0005\n",
      "Epoch [9/10], Step [10000/15000], Loss: 0.0002\n",
      "Epoch [9/10], Step [10100/15000], Loss: 0.0033\n",
      "Epoch [9/10], Step [10200/15000], Loss: 0.0058\n",
      "Epoch [9/10], Step [10300/15000], Loss: 0.0001\n",
      "Epoch [9/10], Step [10400/15000], Loss: 0.0001\n",
      "Epoch [9/10], Step [10500/15000], Loss: 0.0036\n",
      "Epoch [9/10], Step [10600/15000], Loss: 0.0036\n",
      "Epoch [9/10], Step [10700/15000], Loss: 0.0004\n",
      "Epoch [9/10], Step [10800/15000], Loss: 0.0001\n",
      "Epoch [9/10], Step [10900/15000], Loss: 0.0025\n",
      "Epoch [9/10], Step [11000/15000], Loss: 0.0290\n",
      "Epoch [9/10], Step [11100/15000], Loss: 0.0072\n",
      "Epoch [9/10], Step [11200/15000], Loss: 0.0057\n",
      "Epoch [9/10], Step [11300/15000], Loss: 0.0071\n",
      "Epoch [9/10], Step [11400/15000], Loss: 0.9530\n",
      "Epoch [9/10], Step [11500/15000], Loss: 0.0002\n",
      "Epoch [9/10], Step [11600/15000], Loss: 0.0002\n",
      "Epoch [9/10], Step [11700/15000], Loss: 0.0002\n",
      "Epoch [9/10], Step [11800/15000], Loss: 0.0762\n",
      "Epoch [9/10], Step [11900/15000], Loss: 0.0020\n",
      "Epoch [9/10], Step [12000/15000], Loss: 0.0046\n",
      "Epoch [9/10], Step [12100/15000], Loss: 0.0015\n",
      "Epoch [9/10], Step [12200/15000], Loss: 0.0020\n",
      "Epoch [9/10], Step [12300/15000], Loss: 0.0026\n",
      "Epoch [9/10], Step [12400/15000], Loss: 0.0088\n",
      "Epoch [9/10], Step [12500/15000], Loss: 0.0002\n",
      "Epoch [9/10], Step [12600/15000], Loss: 0.0007\n",
      "Epoch [9/10], Step [12700/15000], Loss: 0.0009\n",
      "Epoch [9/10], Step [12800/15000], Loss: 0.0000\n",
      "Epoch [9/10], Step [12900/15000], Loss: 0.0057\n",
      "Epoch [9/10], Step [13000/15000], Loss: 0.0064\n",
      "Epoch [9/10], Step [13100/15000], Loss: 0.0006\n",
      "Epoch [9/10], Step [13200/15000], Loss: 0.0000\n",
      "Epoch [9/10], Step [13300/15000], Loss: 0.0008\n",
      "Epoch [9/10], Step [13400/15000], Loss: 0.0001\n",
      "Epoch [9/10], Step [13500/15000], Loss: 0.0002\n",
      "Epoch [9/10], Step [13600/15000], Loss: 0.0088\n",
      "Epoch [9/10], Step [13700/15000], Loss: 0.0007\n",
      "Epoch [9/10], Step [13800/15000], Loss: 0.0003\n",
      "Epoch [9/10], Step [13900/15000], Loss: 0.0878\n",
      "Epoch [9/10], Step [14000/15000], Loss: 0.1705\n",
      "Epoch [9/10], Step [14100/15000], Loss: 0.0698\n",
      "Epoch [9/10], Step [14200/15000], Loss: 0.1425\n",
      "Epoch [9/10], Step [14300/15000], Loss: 0.0004\n",
      "Epoch [9/10], Step [14400/15000], Loss: 0.0001\n",
      "Epoch [9/10], Step [14500/15000], Loss: 0.1065\n",
      "Epoch [9/10], Step [14600/15000], Loss: 0.0020\n",
      "Epoch [9/10], Step [14700/15000], Loss: 0.0000\n",
      "Epoch [9/10], Step [14800/15000], Loss: 0.0216\n",
      "Epoch [9/10], Step [14900/15000], Loss: 0.0020\n",
      "Epoch [9/10], Step [15000/15000], Loss: 0.0001\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Accuracy of the network on the 10000 test images: 97.66 %\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Epoch [10/10], Step [100/15000], Loss: 0.0053\n",
      "Epoch [10/10], Step [200/15000], Loss: 0.0015\n",
      "Epoch [10/10], Step [300/15000], Loss: 0.0851\n",
      "Epoch [10/10], Step [400/15000], Loss: 0.0003\n",
      "Epoch [10/10], Step [500/15000], Loss: 0.0002\n",
      "Epoch [10/10], Step [600/15000], Loss: 0.0002\n",
      "Epoch [10/10], Step [700/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [800/15000], Loss: 0.0018\n",
      "Epoch [10/10], Step [900/15000], Loss: 0.0195\n",
      "Epoch [10/10], Step [1000/15000], Loss: 0.1572\n",
      "Epoch [10/10], Step [1100/15000], Loss: 0.0011\n",
      "Epoch [10/10], Step [1200/15000], Loss: 0.0003\n",
      "Epoch [10/10], Step [1300/15000], Loss: 0.0003\n",
      "Epoch [10/10], Step [1400/15000], Loss: 0.0054\n",
      "Epoch [10/10], Step [1500/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [1600/15000], Loss: 0.0060\n",
      "Epoch [10/10], Step [1700/15000], Loss: 0.1400\n",
      "Epoch [10/10], Step [1800/15000], Loss: 0.0004\n",
      "Epoch [10/10], Step [1900/15000], Loss: 0.0014\n",
      "Epoch [10/10], Step [2000/15000], Loss: 0.0023\n",
      "Epoch [10/10], Step [2100/15000], Loss: 0.0035\n",
      "Epoch [10/10], Step [2200/15000], Loss: 0.3455\n",
      "Epoch [10/10], Step [2300/15000], Loss: 0.1611\n",
      "Epoch [10/10], Step [2400/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [2500/15000], Loss: 0.0023\n",
      "Epoch [10/10], Step [2600/15000], Loss: 0.4859\n",
      "Epoch [10/10], Step [2700/15000], Loss: 0.0106\n",
      "Epoch [10/10], Step [2800/15000], Loss: 0.0002\n",
      "Epoch [10/10], Step [2900/15000], Loss: 0.0016\n",
      "Epoch [10/10], Step [3000/15000], Loss: 0.0018\n",
      "Epoch [10/10], Step [3100/15000], Loss: 0.0002\n",
      "Epoch [10/10], Step [3200/15000], Loss: 0.0130\n",
      "Epoch [10/10], Step [3300/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [3400/15000], Loss: 0.0075\n",
      "Epoch [10/10], Step [3500/15000], Loss: 0.0012\n",
      "Epoch [10/10], Step [3600/15000], Loss: 0.0089\n",
      "Epoch [10/10], Step [3700/15000], Loss: 0.0058\n",
      "Epoch [10/10], Step [3800/15000], Loss: 0.0003\n",
      "Epoch [10/10], Step [3900/15000], Loss: 0.0090\n",
      "Epoch [10/10], Step [4000/15000], Loss: 0.0004\n",
      "Epoch [10/10], Step [4100/15000], Loss: 0.0003\n",
      "Epoch [10/10], Step [4200/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [4300/15000], Loss: 0.0715\n",
      "Epoch [10/10], Step [4400/15000], Loss: 0.0008\n",
      "Epoch [10/10], Step [4500/15000], Loss: 0.0002\n",
      "Epoch [10/10], Step [4600/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [4700/15000], Loss: 0.0720\n",
      "Epoch [10/10], Step [4800/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [4900/15000], Loss: 0.0002\n",
      "Epoch [10/10], Step [5000/15000], Loss: 0.0203\n",
      "Epoch [10/10], Step [5100/15000], Loss: 0.0006\n",
      "Epoch [10/10], Step [5200/15000], Loss: 0.0032\n",
      "Epoch [10/10], Step [5300/15000], Loss: 0.1045\n",
      "Epoch [10/10], Step [5400/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [5500/15000], Loss: 0.0003\n",
      "Epoch [10/10], Step [5600/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [5700/15000], Loss: 0.0301\n",
      "Epoch [10/10], Step [5800/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [5900/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [6000/15000], Loss: 0.0029\n",
      "Epoch [10/10], Step [6100/15000], Loss: 0.0608\n",
      "Epoch [10/10], Step [6200/15000], Loss: 0.0032\n",
      "Epoch [10/10], Step [6300/15000], Loss: 0.0069\n",
      "Epoch [10/10], Step [6400/15000], Loss: 0.2749\n",
      "Epoch [10/10], Step [6500/15000], Loss: 0.0006\n",
      "Epoch [10/10], Step [6600/15000], Loss: 0.0560\n",
      "Epoch [10/10], Step [6700/15000], Loss: 0.0078\n",
      "Epoch [10/10], Step [6800/15000], Loss: 0.0782\n",
      "Epoch [10/10], Step [6900/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [7000/15000], Loss: 0.0150\n",
      "Epoch [10/10], Step [7100/15000], Loss: 0.0005\n",
      "Epoch [10/10], Step [7200/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [7300/15000], Loss: 0.0662\n",
      "Epoch [10/10], Step [7400/15000], Loss: 0.0114\n",
      "Epoch [10/10], Step [7500/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [7600/15000], Loss: 0.0010\n",
      "Epoch [10/10], Step [7700/15000], Loss: 0.0023\n",
      "Epoch [10/10], Step [7800/15000], Loss: 0.0024\n",
      "Epoch [10/10], Step [7900/15000], Loss: 0.0023\n",
      "Epoch [10/10], Step [8000/15000], Loss: 0.0002\n",
      "Epoch [10/10], Step [8100/15000], Loss: 0.0010\n",
      "Epoch [10/10], Step [8200/15000], Loss: 0.0017\n",
      "Epoch [10/10], Step [8300/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [8400/15000], Loss: 0.0740\n",
      "Epoch [10/10], Step [8500/15000], Loss: 0.0016\n",
      "Epoch [10/10], Step [8600/15000], Loss: 0.0014\n",
      "Epoch [10/10], Step [8700/15000], Loss: 0.0003\n",
      "Epoch [10/10], Step [8800/15000], Loss: 0.0002\n",
      "Epoch [10/10], Step [8900/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [9000/15000], Loss: 0.0011\n",
      "Epoch [10/10], Step [9100/15000], Loss: 0.0124\n",
      "Epoch [10/10], Step [9200/15000], Loss: 0.0002\n",
      "Epoch [10/10], Step [9300/15000], Loss: 0.0068\n",
      "Epoch [10/10], Step [9400/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [9500/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [9600/15000], Loss: 0.0004\n",
      "Epoch [10/10], Step [9700/15000], Loss: 0.0011\n",
      "Epoch [10/10], Step [9800/15000], Loss: 0.0086\n",
      "Epoch [10/10], Step [9900/15000], Loss: 0.0004\n",
      "Epoch [10/10], Step [10000/15000], Loss: 0.0011\n",
      "Epoch [10/10], Step [10100/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [10200/15000], Loss: 0.0023\n",
      "Epoch [10/10], Step [10300/15000], Loss: 0.0053\n",
      "Epoch [10/10], Step [10400/15000], Loss: 0.0136\n",
      "Epoch [10/10], Step [10500/15000], Loss: 0.0006\n",
      "Epoch [10/10], Step [10600/15000], Loss: 0.0195\n",
      "Epoch [10/10], Step [10700/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [10800/15000], Loss: 0.0027\n",
      "Epoch [10/10], Step [10900/15000], Loss: 0.0003\n",
      "Epoch [10/10], Step [11000/15000], Loss: 0.0814\n",
      "Epoch [10/10], Step [11100/15000], Loss: 0.0216\n",
      "Epoch [10/10], Step [11200/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [11300/15000], Loss: 0.0029\n",
      "Epoch [10/10], Step [11400/15000], Loss: 0.0097\n",
      "Epoch [10/10], Step [11500/15000], Loss: 0.0020\n",
      "Epoch [10/10], Step [11600/15000], Loss: 0.0002\n",
      "Epoch [10/10], Step [11700/15000], Loss: 0.0011\n",
      "Epoch [10/10], Step [11800/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [11900/15000], Loss: 0.0002\n",
      "Epoch [10/10], Step [12000/15000], Loss: 0.0002\n",
      "Epoch [10/10], Step [12100/15000], Loss: 0.0004\n",
      "Epoch [10/10], Step [12200/15000], Loss: 0.0016\n",
      "Epoch [10/10], Step [12300/15000], Loss: 0.0121\n",
      "Epoch [10/10], Step [12400/15000], Loss: 0.0006\n",
      "Epoch [10/10], Step [12500/15000], Loss: 0.6405\n",
      "Epoch [10/10], Step [12600/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [12700/15000], Loss: 0.0013\n",
      "Epoch [10/10], Step [12800/15000], Loss: 0.0057\n",
      "Epoch [10/10], Step [12900/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [13000/15000], Loss: 0.1331\n",
      "Epoch [10/10], Step [13100/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [13200/15000], Loss: 0.0022\n",
      "Epoch [10/10], Step [13300/15000], Loss: 0.0026\n",
      "Epoch [10/10], Step [13400/15000], Loss: 0.0013\n",
      "Epoch [10/10], Step [13500/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [13600/15000], Loss: 0.0519\n",
      "Epoch [10/10], Step [13700/15000], Loss: 0.0004\n",
      "Epoch [10/10], Step [13800/15000], Loss: 0.0743\n",
      "Epoch [10/10], Step [13900/15000], Loss: 0.0005\n",
      "Epoch [10/10], Step [14000/15000], Loss: 0.0003\n",
      "Epoch [10/10], Step [14100/15000], Loss: 0.0079\n",
      "Epoch [10/10], Step [14200/15000], Loss: 0.0009\n",
      "Epoch [10/10], Step [14300/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [14400/15000], Loss: 0.0166\n",
      "Epoch [10/10], Step [14500/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [14600/15000], Loss: 0.0006\n",
      "Epoch [10/10], Step [14700/15000], Loss: 0.0001\n",
      "Epoch [10/10], Step [14800/15000], Loss: 0.0057\n",
      "Epoch [10/10], Step [14900/15000], Loss: 0.0000\n",
      "Epoch [10/10], Step [15000/15000], Loss: 1.1576\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Accuracy of the network on the 10000 test images: 97.45 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_size = 28 * 28 \n",
    "hidden_size = 128\n",
    "output_size = 10  \n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 28*28)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.reshape(-1, 28*28)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the network: {} %'.format(100 * correct / total))\n",
    "\n",
    "    model.train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ced63c54952fb711462c5b23e25925157bd2f9b85cfe19b36cd014c2f46d5e7f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
